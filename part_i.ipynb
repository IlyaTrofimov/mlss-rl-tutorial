{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "\n",
    "\"\"\"\n",
    "PointEnv from rllab\n",
    "The goal is to control an agent and get it to the target located at (0,0).\n",
    "At each timestep the agent gets its current location (x,y) as observation, \n",
    "takes an action (dx,dy), and is transitioned to (x+dx, y+dy).\n",
    "\"\"\"\n",
    "\n",
    "class PointEnv():\n",
    "    def reset(self):\n",
    "        self._state = np.random.uniform(-1, 1, size=(2,))\n",
    "        state = np.copy(self._state)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        self._state = self._state + 0.1 * action\n",
    "        x, y = self._state\n",
    "        reward = -(x ** 2 + y ** 2) ** 0.5 - 0.02 * np.sum(action ** 2)\n",
    "        done = abs(x) < 0.01 and abs(y) < 0.01\n",
    "        next_state = np.copy(self._state)\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "\n",
    "# make these smaller to increase the resolution\n",
    "dx, dy = 0.05, 0.05\n",
    "\n",
    "# generate 2 2d grids for the x & y bounds\n",
    "y, x = np.mgrid[slice(-5, 5 + dy, dy),\n",
    "                slice(-5, 5 + dx, dx)]\n",
    "\n",
    "z = -np.sqrt(x**2 + y**2)\n",
    "\n",
    "# x and y are bounds, so z should be the value *inside* those bounds.\n",
    "# Therefore, remove the last value from the z array.\n",
    "z = z[:-1, :-1]\n",
    "levels = MaxNLocator(nbins=15).tick_values(z.min(), z.max())\n",
    "\n",
    "\n",
    "# pick the desired colormap, sensible levels, and define a normalization\n",
    "# instance which takes data values and translates those into levels.\n",
    "cmap = plt.get_cmap('spring')\n",
    "norm = BoundaryNorm(levels, ncolors=cmap.N, clip=True)\n",
    "\n",
    "fig, ax0 = plt.subplots(nrows=1)\n",
    "\n",
    "im = ax0.pcolormesh(x, y, z, cmap=cmap, norm=norm)\n",
    "fig.colorbar(im, ax=ax0)\n",
    "ax0.set_title('Reward')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the policy gradient theorem for continuous domains. Recall the reinforcement learning objective, which we write here in full:\n",
    "\\begin{align}\n",
    "J(\\theta)=\\int p_0(s)\\int \\pi_\\theta(a\\vert s) Q(a,s) da ds. \\label{eq:rl_objective}\n",
    "\\end{align}\n",
    "Just like in the discrete case (https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf , (Sutton et al. 2000)), we can take derivatives of objective with respect to the policy parameters $\\theta$, yielding:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)=\\int\\pi_\\theta(a_t\\vert s_t)\\int \\nabla_\\theta\\pi_\\theta(a\\vert s) Q(a,s) da ds ,\\label{eq:policy_gradient_theorem}\n",
    "\\end{align} \n",
    "where $\\pi_\\theta(a_t\\vert s_t)$ is the discounted ergodic occupancy measure. Using the log-derivative trick, we write \\cref{eq:policy_gradient_theorem} as an expectation over actions:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)=&\\int\\pi_\\theta(a_t\\vert s_t)\\int\\pi_\\theta(a\\vert s) \\nabla_\\theta\\log\\pi_\\theta(a\\vert s) Q(a,s) da ds,\\\\ =&\\mathbb{E}_{s\\sim\\pi_\\theta(a_t\\vert s_t),a\\sim\\pi_\\theta(a\\vert s)}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s) Q(a,s)\\Big].\n",
    "\\end{align}\n",
    "We then approximate the expectation by using $N$ samples from the environment under our policy $\\pi_\\theta(a\\vert s)$, giving:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)\\approx&\\frac{1}{N}\\sum_{n=0}^{N-1}\\sum_{t=0}^{T_N}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t) Q(a_t,s_t).\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE and Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Monte-Carlo approximator for the action-value function, $Q(a_t,s_t)\\approx R_t\\triangleq\\sum_{i=t}^T r(a_i,s_i)$, we recover the REINFORCE (http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf ,(Williams 1992)) algorithm:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)\\approx&\\frac{1}{N}\\sum_{n=0}^{N-1}\\sum_{t=0}^{T_N}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t) R_t.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise I.1: Implementing REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose our policy to be a linear Gaussian policy with parameters $\\theta$. Given state $s$, we can define some features $\\phi(s)$ and sample an action $a \\sim N(\\phi(s)^T \\theta, \\sigma^2)$. Our PointEnv environment is simple enough that we can use $\\phi(s) = s$. Note that $\\sigma^2$ can also depend on $s$, but we have kept it constant in Part I for simplicity. Policy gradient algorithms use the update rule $\\theta' = \\theta + \\alpha \\nabla_\\theta J(\\pi)$, where $\\alpha$ is the learning rate and $J(\\pi)$ is the expected return of the policy.\n",
    "\n",
    "PROBLEM I.1) In the function \"get\\_action\\_and\\_grad()\", sample an action from $\\pi_\\theta(a\\vert s)$ and compute the corresponding value of $\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gauss_Policy():\n",
    "    def __init__(self):\n",
    "        self.action_dim = 2\n",
    "        self.theta = 0.5 * np.ones(4)\n",
    "        # theta here is a length 4 array instead of a matrix for ease of processing\n",
    "        # Think of treating theta as a 2x2 matrix and then flatenning it, which gives us:\n",
    "        # action[0] = state[0]*[theta[0], theta[1]]\n",
    "        # action[1] = state[1]*[theta[2], theta[3]]\n",
    "\n",
    "    def get_action_and_grad(self, state):\n",
    "        # Exercise I.1:\n",
    "        mean_act = ...\n",
    "        sampled_act = ...\n",
    "        grad_log_pi = ...\n",
    "        return sampled_act, grad_log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function collects some trajectories, given a policy\n",
    "def gather_paths(env, policy, num_paths, max_ts=500):\n",
    "    paths = []\n",
    "    for i in range(num_paths):\n",
    "        ts = 0\n",
    "        states = []\n",
    "        act = []\n",
    "        grads = []\n",
    "        rwd = []\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done and ts < max_ts:\n",
    "            a, grad_a = policy.get_action_and_grad(s)\n",
    "            next_s, r, done = env.step(a)\n",
    "            states += [s]\n",
    "            act += [a]\n",
    "            rwd += [r]\n",
    "            grads += [grad_a]\n",
    "            s = next_s\n",
    "            ts += 1\n",
    "        path = {'states': np.array(states),\n",
    "                'actions': np.array(act),\n",
    "                'grad_log_pi': np.array(grads),\n",
    "                'rwd': np.array(rwd)}\n",
    "        paths += [path]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise I.2: Using Baselines to Reduce Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subtract a baseline $V(s)$ from the action-value function, which does not affect the overall gradient, but will reduce variance in the gradient update. To see why, as $V_\\omega(s)\\triangleq\\mathbb{E}_{a\\sim\\pi_\\theta(a\\vert s)}[Q_\\omega(a,s)]$, the quantity $Q_\\omega(a,s)-V\\omega(s)$ will only be positive for $Q_\\omega(a,s)>V_\\omega(s)$, hence the gradient updates will therefore only be scaled by a positive value when an action has greater reward than the average return. For these reasons, we call $A_\\omega(a,s)=Q_\\omega(a,s)-V_\\omega(s)$ the advantage. For REINFORCE, $A_\\omega(a_t,s_t)=R_t-V_\\omega(s_t)$. Just like we define features $\\phi(s)$ for the policy, we can define some features $\\psi(s,t)$ for the value function. Our features are going to be $\\phi(s,t)=[s, s^2, t, t^2, t^3, 1]$ and we will approximate the value function as a linear function of these features, $V_\\omega (s_t)=\\omega^T\\phi(s,t)$\n",
    "\n",
    "PROBLEM I.2a) Given some sampled trajectories, fitting the value function is a linear regression problem.  The targets for our linear regression problem will therefore be the returns. The features have been implemented in the function \"baselines()\" you are required to compute the regression coefficients. Hint: The function \"np.linalg.lstsq\" may be of use.\n",
    "\n",
    "PROBLEM I.2b) Now calculate the value for each state in \\code{path} using the newly learnt coefficients and save it to \"path[`value']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(paths):\n",
    "    path_features = []\n",
    "    for path in paths:\n",
    "        s = path[\"states\"]\n",
    "        l = len(path[\"rwd\"])\n",
    "        al = np.arange(l).reshape(-1, 1) / 100.0\n",
    "        path_features += [np.concatenate([s, s ** 2, al, al ** 2, al ** 3, np.ones((l, 1))], axis=1)]\n",
    "    ft = np.concatenate([el for el in path_features])\n",
    "    targets = np.concatenate([el['returns'] for el in paths])\n",
    "\n",
    "    # Exercise I.2(a): Compute the regression coefficents\n",
    "    coeffs = ...\n",
    "    # Exercise I.2(b): Calculate the values for each state\n",
    "    for i, path in enumerate(paths):\n",
    "        path['value'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paths(paths, discount_rate=1):\n",
    "    grads = []\n",
    "    for path in paths:\n",
    "        # Exercise I.3a: Implement the discounted return\n",
    "        # Hint: This can be done in one line using lfilter from scipy.signal,\n",
    "        # but it might be much easier to write a separate function for this\n",
    "        path['returns'] = scipy.signal.lfilter([1], [1, float(-discount_rate)], path['rwd'][::-1], axis=0)[::-1]\n",
    "    baseline(paths)\n",
    "    for path in paths:\n",
    "        path['adv'] = path['returns'] - path['value']\n",
    "        rets_for_grads = np.atleast_2d(path['adv']).T\n",
    "        rets_for_grads = np.repeat(rets_for_grads, path['grad_log_pi'].shape[1], axis=1)\n",
    "        path['grads'] = path['grad_log_pi'] * rets_for_grads\n",
    "        grads += [np.sum(path['grads'], axis=0)]\n",
    "    grads = np.sum(grads, axis=0) / len(paths)\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algo\n",
    "env = PointEnv()\n",
    "alpha = 0.01\n",
    "traj_len = 50\n",
    "perf_stats = []\n",
    "\n",
    "def run_algo(env, alpha, gamma, traj_len, num_itr=200, runs=10):\n",
    "    rwd = np.zeros((num_itr, runs))\n",
    "    for st in range(runs):\n",
    "        policy = Gauss_Policy()\n",
    "        for i in range(num_itr):\n",
    "            paths = gather_paths(env, policy, max_ts=traj_len, num_paths=5)\n",
    "            rwd[i, st] = np.mean([np.sum(path['rwd']) for path in paths])\n",
    "            grads = process_paths(paths, discount_rate=gamma)\n",
    "            policy.theta += alpha * grads\n",
    "    perf_stats = {'gamma': gamma,\n",
    "                  'mean_rwd': np.mean(rwd, axis=1),\n",
    "                  'std_err': np.std(rwd, axis=1) / np.sqrt(runs)}\n",
    "    return perf_stats\n",
    "\n",
    "gamma = [0.99, 0.995, 1.0]\n",
    "for g in gamma:\n",
    "    print(\"Starting algorithm with gamma:\", g)\n",
    "    perf_stats += [run_algo(env, alpha, gamma=g, traj_len=traj_len)]\n",
    "\n",
    "# And plot the results\n",
    "for el in perf_stats:\n",
    "    plt.plot(el['mean_rwd'], label='discount factor = ' + str(el['gamma']))\n",
    "    plt.fill_between(np.arange(len(el['mean_rwd'])), el['mean_rwd'] + el['std_err'], el['mean_rwd'] - el['std_err'],\n",
    "                     alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Returns')\n",
    "plt.xlim([0, 200])\n",
    "plt.show()\n",
    "\n",
    "# Exercise I.3(b): Run the algo again, but with traj_len=500.\n",
    "# Does the relative performance of learning using discount factors change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
