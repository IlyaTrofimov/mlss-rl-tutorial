{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch.optim import Adam\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torch import nn\n",
    "from storage import Buffer\n",
    "from torch.distributions import Uniform\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as f\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Pendulum problem](https://www.youtube.com/watch?v=1IoN6yCb21s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### networks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "    ):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, activation):\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        activation = self.fc3(activation)\n",
    "\n",
    "        return activation\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_size,\n",
    "            input_size,\n",
    "            max_action,\n",
    "            min_action,\n",
    "            soft_clamp_function=None\n",
    "    ):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.soft_clamp_function = soft_clamp_function\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.max_log_sig = 2\n",
    "        self.min_log_sig = -20\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.mu_head = nn.Linear(256, self.output_size)\n",
    "        self.log_sig_head = nn.Linear(256, self.output_size)\n",
    "\n",
    "    def forward(self, activation):\n",
    "\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        mu = self.mu_head(activation)\n",
    "        log_sig = self.log_sig_head(activation)\n",
    "        log_sig = log_sig.clamp(min=self.min_log_sig, max=self.max_log_sig)\n",
    "        sig = th.exp(log_sig)\n",
    "\n",
    "        return mu, sig\n",
    "\n",
    "    def get_action(self, state, eval_deterministic=False):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        if eval_deterministic:\n",
    "            action = mu\n",
    "        else:\n",
    "            gauss = Normal(loc=mu, scale=sig)\n",
    "            action = gauss.sample()\n",
    "            action.detach()\n",
    "\n",
    "        action = self.max_action * th.tanh(action / self.max_action)\n",
    "        return action\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        action = gauss.sample()\n",
    "        action.detach()\n",
    "        action = action.clamp(min=self.min_action, max=self.max_action)\n",
    "        log_prob = gauss.log_prob(action)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def r_sample(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        loc = th.zeros(size=[state.shape[0], 1], dtype=th.float32)\n",
    "        scale = loc + 1.0\n",
    "        unit_gauss = Normal(loc=loc, scale=scale)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        epsilon = unit_gauss.sample()\n",
    "        action = mu + sig * epsilon\n",
    "        action = action.requires_grad_()\n",
    "        action = self.max_action * th.tanh(action / self.max_action)\n",
    "        log_prob = gauss.log_prob(action.data)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "class TanhGaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_size,\n",
    "            input_size,\n",
    "            max_action,\n",
    "            min_action,\n",
    "            soft_clamp_function=None\n",
    "    ):\n",
    "        super(TanhGaussianPolicy, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.soft_clamp_function = soft_clamp_function\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.max_log_sig = 2\n",
    "        self.min_log_sig = -20\n",
    "        self.a_diff = 0.5 * (self.max_action - self.min_action)\n",
    "        self.a_shift = 0.5 * (self.max_action + self.min_action)\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.mu_head = nn.Linear(256, self.output_size)\n",
    "        self.log_sig_head = nn.Linear(256, self.output_size)\n",
    "\n",
    "    def forward(self, activation):\n",
    "\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        mu = self.mu_head(activation)\n",
    "        log_sig = self.log_sig_head(activation)\n",
    "        log_sig = log_sig.clamp(min=self.min_log_sig, max=self.max_log_sig)\n",
    "        sig = th.exp(log_sig)\n",
    "\n",
    "        return mu, sig\n",
    "\n",
    "    def tanh_function(self, a):\n",
    "\n",
    "        a = self.a_diff * th.tanh(a / self.a_diff) + self.a_shift\n",
    "\n",
    "        return a\n",
    "\n",
    "    def tanh_function_derivative(self, a):\n",
    "\n",
    "        return 1 - (th.tanh(a / self.a_diff) ** 2) + self.epsilon\n",
    "\n",
    "    def get_action(self, state, eval_deterministic=False):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        if eval_deterministic:\n",
    "            action = self.tanh_function(mu)\n",
    "        else:\n",
    "            gauss = Normal(loc=mu, scale=sig)\n",
    "            action = gauss.sample()\n",
    "            action = self.tanh_function(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        pre_tanh_action = gauss.sample()\n",
    "        pre_tanh_log_prob = gauss.log_prob(pre_tanh_action)\n",
    "        action = self.tanh_function(pre_tanh_action)\n",
    "        log_prob = pre_tanh_log_prob - th.log(self.tanh_function_derivative(pre_tanh_action))\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def r_sample(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        loc = th.zeros(size=[state.shape[0], 1], dtype=th.float32)\n",
    "        scale = loc + 1.0\n",
    "        unit_gauss = Normal(loc=loc, scale=scale)\n",
    "        epsilon = unit_gauss.sample()\n",
    "        pre_tanh_action = mu + sig * epsilon\n",
    "        action = self.tanh_function(pre_tanh_action)\n",
    "\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        pre_tanh_log_prob = gauss.log_prob(pre_tanh_action)\n",
    "        log_prob = pre_tanh_log_prob - th.log(self.tanh_function_derivative(pre_tanh_action))\n",
    "\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### algorithms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy,\n",
    "            qf,\n",
    "            env,\n",
    "            discount,\n",
    "            qf_optimiser,\n",
    "            policy_optimiser,\n",
    "            max_evaluation_episode_length=200,\n",
    "            num_evaluation_episodes=5,\n",
    "            num_training_episode_steps=1000,\n",
    "            batch_size=128,\n",
    "            buffer_size = 10000,\n",
    "            eval_deterministic = True,\n",
    "            training_on_policy = False,\n",
    "            vf=None,\n",
    "            vf_optimiser=None\n",
    "    ):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.qf = qf\n",
    "        self.vf = vf\n",
    "        self.target_vf = deepcopy(vf)\n",
    "        self.tau = 1e-2\n",
    "        self.vf_optimiser = vf_optimiser\n",
    "        self.qf_optimiser = qf_optimiser\n",
    "        self.policy_optimiser = policy_optimiser\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.max_evaluation_episode_length = max_evaluation_episode_length\n",
    "        self.num_evaluation_episodes = num_evaluation_episodes\n",
    "        self.num_training_episode_steps = num_training_episode_steps\n",
    "        self.training_on_policy = training_on_policy\n",
    "        self.buffer = Buffer(buffer_size=buffer_size)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.pretraining_policy = Uniform(high=th.Tensor([policy.max_action]), low=th.Tensor([policy.min_action]))\n",
    "        self.eval_deterministic = eval_deterministic\n",
    "\n",
    "        self.R_av = None\n",
    "        self.R_tot = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = th.from_numpy(self.env.reset()).float()\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        for _ in range(self.num_evaluation_episodes):\n",
    "            state = th.from_numpy(self.env.reset()).float()\n",
    "            episode_return = 0\n",
    "\n",
    "            for _ in range(self.max_evaluation_episode_length):\n",
    "                action = self.policy.get_action(state, self.eval_deterministic)\n",
    "                action = np.array([action.item()])\n",
    "\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                state = th.from_numpy(state).float()\n",
    "\n",
    "                episode_return += reward\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "\n",
    "            total_return += episode_return\n",
    "\n",
    "        self.average_return = total_return/self.num_evaluation_episodes\n",
    "\n",
    "    def sample_episode(self, exploration_mode=False):\n",
    "\n",
    "        self.reset()\n",
    "        state = self.state\n",
    "\n",
    "        for _ in range(self.num_training_episode_steps):\n",
    "\n",
    "            if exploration_mode:\n",
    "                action = self.pretraining_policy.sample()\n",
    "            else:\n",
    "                action = self.policy.get_action(state)\n",
    "            next_state, reward, terminal, _ = self.env.step(action.numpy())\n",
    "            next_state = th.from_numpy(next_state).float()\n",
    "            reward = th.Tensor([reward])\n",
    "            terminal = th.Tensor([terminal])\n",
    "\n",
    "            self.buffer.add(state=state,\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_state=next_state,\n",
    "                            terminal=terminal)\n",
    "\n",
    "            state = next_state\n",
    "            if terminal:\n",
    "                self.reset()\n",
    "                state = self.state\n",
    "\n",
    "    def env_step(self):\n",
    "\n",
    "        state = self.state\n",
    "        action = self.policy.get_action(state)\n",
    "        next_state, reward, terminal, _ = self.env.step(action.numpy())\n",
    "        next_state = th.from_numpy(next_state).float()\n",
    "        reward = th.Tensor([reward])\n",
    "        terminal = th.Tensor([terminal])\n",
    "\n",
    "        self.buffer.add(state=state,\n",
    "                        action=action,\n",
    "                        reward=reward,\n",
    "                        next_state=next_state,\n",
    "                        terminal=terminal)\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "    def train_score(self):\n",
    "        if self.training_on_policy:\n",
    "            batch = self.buffer.whole_batch()\n",
    "            self.buffer.clear()\n",
    "        else:\n",
    "            batch = self.buffer.random_batch(self.batch_size)\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        next_states = batch['next_states']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        new_actions, log_pis = self.policy.get_action_and_log_prob(states)\n",
    "        values = self.vf(states)\n",
    "        state_actions = th.cat((states, actions), 1)\n",
    "        q_values = self.qf(state_actions)\n",
    "        next_values = self.target_vf(next_states)\n",
    "        new_state_actions = th.cat((states, new_actions), 1)\n",
    "        new_q_values = self.qf(new_state_actions)\n",
    "\n",
    "        \"\"\"\n",
    "        Value (Critic) Losses:\n",
    "        \"\"\"\n",
    "        v_targets = new_q_values\n",
    "        vf_loss = (v_targets.detach() - values).pow(2).mean()\n",
    "\n",
    "        q_targets = rewards + self.discount * (1 - terminals) * next_values\n",
    "        qf_loss = (q_targets.detach() - q_values).pow(2).mean()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Policy (Actor) Losses: TO COMPLETE IN EXERCISE II.2b\n",
    "        \"\"\"\n",
    "        advantage = new_q_values - values\n",
    "        policy_loss = (log_pis * (log_pis - advantage.detach())).mean()\n",
    "\n",
    "        \"\"\"\n",
    "        Gradient Updates\n",
    "        \"\"\"\n",
    "        self.qf_optimiser.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimiser.step()\n",
    "\n",
    "        self.vf_optimiser.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimiser.step()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "    def train_reparametrisation(self):\n",
    "\n",
    "        if self.training_on_policy:\n",
    "            batch = self.buffer.whole_batch()\n",
    "            self.buffer.clear()\n",
    "        else:\n",
    "            batch = self.buffer.random_batch(self.batch_size)\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        next_states = batch['next_states']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        state_actions = th.cat((states, actions), 1)\n",
    "        q_pred = self.qf(state_actions)\n",
    "        v_pred = self.vf(states)\n",
    "        new_actions, log_pis = self.policy.r_sample(states)\n",
    "\n",
    "        \"\"\"\n",
    "        Value (Critic) Losses:\n",
    "        \"\"\"\n",
    "        target_v_values = self.target_vf(next_states)\n",
    "        q_target = rewards + (1. - terminals) * self.discount * target_v_values\n",
    "        qf_loss = self.loss(q_pred, q_target.detach())\n",
    "\n",
    "        new_state_actions = th.cat((states, new_actions), 1)\n",
    "        q_new_actions = self.qf(new_state_actions)\n",
    "        v_target = q_new_actions\n",
    "        vf_loss = self.loss(v_pred, v_target.detach())\n",
    "\n",
    "        \"\"\"\n",
    "        Policy (Actor) Loss: TO COMPLETE IN EXERCISE II.3c     \n",
    "        \"\"\"\n",
    "        policy_loss = (log_pis - q_new_actions).mean()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Gradient Updates\n",
    "        \"\"\"\n",
    "        self.qf_optimiser.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimiser.step()\n",
    "\n",
    "        self.vf_optimiser.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimiser.step()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, param in zip(self.target_vf.parameters(), self.vf.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(s):\n",
    "\n",
    "    policy = TanhGaussianPolicy(\n",
    "        max_action=max_action,\n",
    "        min_action=min_action,\n",
    "        input_size=state_dim,\n",
    "        output_size=action_dim\n",
    "    )\n",
    "    policy_optimiser = Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    qf = ValueFunction(\n",
    "        input_size=state_dim + action_dim,\n",
    "    )\n",
    "    qf_optimiser = Adam(qf.parameters(), lr=lr)\n",
    "\n",
    "    vf = ValueFunction(\n",
    "        input_size=state_dim,\n",
    "    )\n",
    "    vf_optimiser = Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "    algorithm = ActorCritic(\n",
    "        policy=policy,\n",
    "        policy_optimiser=policy_optimiser,\n",
    "        qf=qf,\n",
    "        qf_optimiser=qf_optimiser,\n",
    "        vf=vf,\n",
    "        vf_optimiser=vf_optimiser,\n",
    "        env=env,\n",
    "        discount=discount\n",
    "    )\n",
    "\n",
    "    for _ in range(num_pretrain_episodes):\n",
    "        algorithm.sample_episode(exploration_mode=True)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        algorithm.evaluate()\n",
    "        print('Epoch: {}, Average Test Return: {}'.format(e, algorithm.average_return))\n",
    "        results[e, s] = algorithm.average_return\n",
    "        algorithm.reset()\n",
    "\n",
    "        for n in range(num_training_updates_per_epoch):\n",
    "            algorithm.env_step()\n",
    "            if training_mode == 'Score':\n",
    "                algorithm.train_score()\n",
    "            else:\n",
    "                algorithm.train_reparametrisation()\n",
    "\n",
    "\n",
    "    algorithm.evaluate(render=render_agent)\n",
    "    print('Epoch: {}, Average Test Return: {}'.format(e+1, algorithm.average_return))\n",
    "    results[e+1, s] = algorithm.average_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(mean, std):\n",
    "    with open('PartII_{}_{}_trials_{}_epochs.csv'.format(training_mode, number_of_trials, num_epochs), 'w') as resultFile:\n",
    "        wr = csv.writer(resultFile)\n",
    "        mean.insert(0, 'Mean Returns')\n",
    "        std.insert(0, 'Standard Deviation')\n",
    "        data = [mean, std]\n",
    "        data = list(map(list, zip(*data)))\n",
    "        wr.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Environment\n",
    "env = gym.make(\"Pendulum-v0\").unwrapped\n",
    "max_action = env.action_space.high[0]\n",
    "min_action = env.action_space.low[0]\n",
    "state_dim = int(env.observation_space.shape[0])\n",
    "action_dim = int(env.action_space.shape[0])\n",
    "# Options: 'Score' trains the policy using the score function and 'Reparam' trains using the reparametrisation trick.\n",
    "training_mode = 'Reparam' \n",
    "number_of_trials = 1\n",
    "save_data = True\n",
    "render_agent = False\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 5\n",
    "num_pretrain_episodes = 1\n",
    "num_training_updates_per_epoch = 200\n",
    "lr = 3e-4\n",
    "discount = 0.99\n",
    "\n",
    "# Initialise data logger\n",
    "results = np.zeros((num_epochs + 1, number_of_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Test Return: -1392.5659115334288\n",
      "Epoch: 1, Average Test Return: -1523.834288354346\n",
      "Epoch: 2, Average Test Return: -1712.4369300894352\n",
      "Epoch: 3, Average Test Return: -1672.739151908319\n",
      "Epoch: 4, Average Test Return: -1569.5727207161713\n",
      "Epoch: 5, Average Test Return: -1231.856655457303\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for s in range(number_of_trials):\n",
    "        th.manual_seed(s+1)\n",
    "        run_experiment(s)\n",
    "\n",
    "    mean_returns = results.mean(axis=1)\n",
    "    std_returns = results.std(axis=1)\n",
    "    training_steps = np.linspace(0, num_epochs * num_training_updates_per_epoch, num=num_epochs+1)\n",
    "    plt.plot(training_steps, mean_returns)\n",
    "    plt.fill_between(training_steps, mean_returns + std_returns, mean_returns - std_returns, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    if save_data:\n",
    "        write_to_file(mean_returns.tolist(), std_returns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
