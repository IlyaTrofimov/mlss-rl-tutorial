{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch.optim import Adam\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torch import nn\n",
    "from storage import Buffer\n",
    "from torch.distributions import Uniform\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as f\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Pendulum problem](https://www.youtube.com/watch?v=1IoN6yCb21s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### networks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "    ):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, activation):\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        activation = self.fc3(activation)\n",
    "\n",
    "        return activation\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_size,\n",
    "            input_size,\n",
    "            max_action,\n",
    "            min_action,\n",
    "            soft_clamp_function=None\n",
    "    ):\n",
    "        super(GaussianPolicy, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.soft_clamp_function = soft_clamp_function\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.max_log_sig = 2\n",
    "        self.min_log_sig = -20\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.mu_head = nn.Linear(256, self.output_size)\n",
    "        self.log_sig_head = nn.Linear(256, self.output_size)\n",
    "\n",
    "    def forward(self, activation):\n",
    "\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        mu = self.mu_head(activation)\n",
    "        log_sig = self.log_sig_head(activation)\n",
    "        log_sig = log_sig.clamp(min=self.min_log_sig, max=self.max_log_sig)\n",
    "        sig = th.exp(log_sig)\n",
    "\n",
    "        return mu, sig\n",
    "\n",
    "    def get_action(self, state, eval_deterministic=False):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        if eval_deterministic:\n",
    "            action = mu\n",
    "        else:\n",
    "            gauss = Normal(loc=mu, scale=sig)\n",
    "            action = gauss.sample()\n",
    "            action.detach()\n",
    "\n",
    "        action = self.max_action * th.tanh(action / self.max_action)\n",
    "        return action\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        action = gauss.sample()\n",
    "        action.detach()\n",
    "        action = action.clamp(min=self.min_action, max=self.max_action)\n",
    "        log_prob = gauss.log_prob(action)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def r_sample(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        loc = th.zeros(size=[state.shape[0], 1], dtype=th.float32)\n",
    "        scale = loc + 1.0\n",
    "        unit_gauss = Normal(loc=loc, scale=scale)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        epsilon = unit_gauss.sample()\n",
    "        action = mu + sig * epsilon\n",
    "        action = action.requires_grad_()\n",
    "        action = self.max_action * th.tanh(action / self.max_action)\n",
    "        log_prob = gauss.log_prob(action.data)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class TanhGaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_size,\n",
    "            input_size,\n",
    "            max_action,\n",
    "            min_action,\n",
    "            soft_clamp_function=None\n",
    "    ):\n",
    "        super(TanhGaussianPolicy, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.soft_clamp_function = soft_clamp_function\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.max_log_sig = 2\n",
    "        self.min_log_sig = -20\n",
    "        self.a_diff = 0.5 * (self.max_action - self.min_action)\n",
    "        self.a_shift = 0.5 * (self.max_action + self.min_action)\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.mu_head = nn.Linear(256, self.output_size)\n",
    "        self.log_sig_head = nn.Linear(256, self.output_size)\n",
    "\n",
    "    def forward(self, activation):\n",
    "\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        mu = self.mu_head(activation)\n",
    "        log_sig = self.log_sig_head(activation)\n",
    "        log_sig = log_sig.clamp(min=self.min_log_sig, max=self.max_log_sig)\n",
    "        sig = th.exp(log_sig)\n",
    "\n",
    "        return mu, sig\n",
    "\n",
    "    def tanh_function(self, a):\n",
    "\n",
    "        a = self.a_diff * th.tanh(a / self.a_diff) + self.a_shift\n",
    "\n",
    "        return a\n",
    "\n",
    "    def tanh_function_derivative(self, a):\n",
    "\n",
    "        return 1 - (th.tanh(a / self.a_diff) ** 2) + self.epsilon\n",
    "\n",
    "    def get_action(self, state, eval_deterministic=False):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        if eval_deterministic:\n",
    "            action = self.tanh_function(mu)\n",
    "        else:\n",
    "            gauss = Normal(loc=mu, scale=sig)\n",
    "            action = gauss.sample()\n",
    "            action = self.tanh_function(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        pre_tanh_action = gauss.sample()\n",
    "        pre_tanh_log_prob = gauss.log_prob(pre_tanh_action)\n",
    "        action = self.tanh_function(pre_tanh_action)\n",
    "        log_prob = pre_tanh_log_prob - th.log(self.tanh_function_derivative(pre_tanh_action))\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def r_sample(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        loc = th.zeros(size=[state.shape[0], 1], dtype=th.float32)\n",
    "        scale = loc + 1.0\n",
    "        unit_gauss = Normal(loc=loc, scale=scale)\n",
    "        epsilon = unit_gauss.sample()\n",
    "        pre_tanh_action = mu + sig * epsilon\n",
    "        action = self.tanh_function(pre_tanh_action)\n",
    "\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        pre_tanh_log_prob = gauss.log_prob(pre_tanh_action)\n",
    "        log_prob = pre_tanh_log_prob - th.log(self.tanh_function_derivative(pre_tanh_action))\n",
    "\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### algorithms.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy,\n",
    "            qf,\n",
    "            env,\n",
    "            discount,\n",
    "            qf_optimiser,\n",
    "            policy_optimiser,\n",
    "            max_evaluation_episode_length=200,\n",
    "            num_evaluation_episodes=5,\n",
    "            num_training_episode_steps=1000,\n",
    "            batch_size=128,\n",
    "            buffer_size = 10000,\n",
    "            eval_deterministic = True,\n",
    "            training_on_policy = False,\n",
    "            vf=None,\n",
    "            vf_optimiser=None\n",
    "    ):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.qf = qf\n",
    "        self.vf = vf\n",
    "        self.target_vf = deepcopy(vf)\n",
    "        self.tau = 1e-2\n",
    "        self.vf_optimiser = vf_optimiser\n",
    "        self.qf_optimiser = qf_optimiser\n",
    "        self.policy_optimiser = policy_optimiser\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.max_evaluation_episode_length = max_evaluation_episode_length\n",
    "        self.num_evaluation_episodes = num_evaluation_episodes\n",
    "        self.num_training_episode_steps = num_training_episode_steps\n",
    "        self.training_on_policy = training_on_policy\n",
    "        self.buffer = Buffer(buffer_size=buffer_size)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.pretraining_policy = Uniform(high=th.Tensor([policy.max_action]), low=th.Tensor([policy.min_action]))\n",
    "        self.eval_deterministic = eval_deterministic\n",
    "\n",
    "        self.R_av = None\n",
    "        self.R_tot = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = th.from_numpy(self.env.reset()).float()\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        for _ in range(self.num_evaluation_episodes):\n",
    "            state = th.from_numpy(self.env.reset()).float()\n",
    "            episode_return = 0\n",
    "\n",
    "            for _ in range(self.max_evaluation_episode_length):\n",
    "                action = self.policy.get_action(state, self.eval_deterministic)\n",
    "                action = np.array([action.item()])\n",
    "\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                state = th.from_numpy(state).float()\n",
    "\n",
    "                episode_return += reward\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "\n",
    "            total_return += episode_return\n",
    "\n",
    "        self.average_return = total_return/self.num_evaluation_episodes\n",
    "\n",
    "    def sample_episode(self, exploration_mode=False):\n",
    "\n",
    "        self.reset()\n",
    "        state = self.state\n",
    "\n",
    "        for _ in range(self.num_training_episode_steps):\n",
    "\n",
    "            if exploration_mode:\n",
    "                action = self.pretraining_policy.sample()\n",
    "            else:\n",
    "                action = self.policy.get_action(state)\n",
    "            next_state, reward, terminal, _ = self.env.step(action.numpy())\n",
    "            next_state = th.from_numpy(next_state).float()\n",
    "            reward = th.Tensor([reward])\n",
    "            terminal = th.Tensor([terminal])\n",
    "\n",
    "            self.buffer.add(state=state,\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_state=next_state,\n",
    "                            terminal=terminal)\n",
    "\n",
    "            state = next_state\n",
    "            if terminal:\n",
    "                self.reset()\n",
    "                state = self.state\n",
    "\n",
    "    def env_step(self):\n",
    "\n",
    "        state = self.state\n",
    "        action = self.policy.get_action(state)\n",
    "        next_state, reward, terminal, _ = self.env.step(action.numpy())\n",
    "        next_state = th.from_numpy(next_state).float()\n",
    "        reward = th.Tensor([reward])\n",
    "        terminal = th.Tensor([terminal])\n",
    "\n",
    "        self.buffer.add(state=state,\n",
    "                        action=action,\n",
    "                        reward=reward,\n",
    "                        next_state=next_state,\n",
    "                        terminal=terminal)\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "    def train_score(self):\n",
    "        if self.training_on_policy:\n",
    "            batch = self.buffer.whole_batch()\n",
    "            self.buffer.clear()\n",
    "        else:\n",
    "            batch = self.buffer.random_batch(self.batch_size)\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        next_states = batch['next_states']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        new_actions, log_pis = self.policy.get_action_and_log_prob(states)\n",
    "        values = self.vf(states)\n",
    "        state_actions = th.cat((states, actions), 1)\n",
    "        q_values = self.qf(state_actions)\n",
    "        next_values = self.target_vf(next_states)\n",
    "        new_state_actions = th.cat((states, new_actions), 1)\n",
    "        new_q_values = self.qf(new_state_actions)\n",
    "\n",
    "        \"\"\"\n",
    "        Value (Critic) Losses:\n",
    "        \"\"\"\n",
    "        v_targets = new_q_values\n",
    "        vf_loss = (v_targets.detach() - values).pow(2).mean()\n",
    "\n",
    "        q_targets = rewards + self.discount * (1 - terminals) * next_values\n",
    "        qf_loss = (q_targets.detach() - q_values).pow(2).mean()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Policy (Actor) Losses: TO COMPLETE IN EXERCISE II.2b\n",
    "        \"\"\"\n",
    "        advantage = new_q_values - values\n",
    "        policy_loss = (log_pis * (log_pis - advantage.detach())).mean()\n",
    "\n",
    "        \"\"\"\n",
    "        Gradient Updates\n",
    "        \"\"\"\n",
    "        self.qf_optimiser.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimiser.step()\n",
    "\n",
    "        self.vf_optimiser.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimiser.step()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "    def train_reparametrisation(self):\n",
    "\n",
    "        if self.training_on_policy:\n",
    "            batch = self.buffer.whole_batch()\n",
    "            self.buffer.clear()\n",
    "        else:\n",
    "            batch = self.buffer.random_batch(self.batch_size)\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        next_states = batch['next_states']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        state_actions = th.cat((states, actions), 1)\n",
    "        q_pred = self.qf(state_actions)\n",
    "        v_pred = self.vf(states)\n",
    "        new_actions, log_pis = self.policy.r_sample(states)\n",
    "\n",
    "        \"\"\"\n",
    "        Value (Critic) Losses:\n",
    "        \"\"\"\n",
    "        target_v_values = self.target_vf(next_states)\n",
    "        q_target = rewards + (1. - terminals) * self.discount * target_v_values\n",
    "        qf_loss = self.loss(q_pred, q_target.detach())\n",
    "\n",
    "        new_state_actions = th.cat((states, new_actions), 1)\n",
    "        q_new_actions = self.qf(new_state_actions)\n",
    "        v_target = q_new_actions\n",
    "        vf_loss = self.loss(v_pred, v_target.detach())\n",
    "\n",
    "        \"\"\"\n",
    "        Policy (Actor) Loss: TO COMPLETE IN EXERCISE II.3c     \n",
    "        \"\"\"\n",
    "        policy_loss = (log_pis - q_new_actions).mean()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Gradient Updates\n",
    "        \"\"\"\n",
    "        self.qf_optimiser.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimiser.step()\n",
    "\n",
    "        self.vf_optimiser.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimiser.step()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, param in zip(self.target_vf.parameters(), self.vf.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Environment\n",
    "env = gym.make(\"Pendulum-v0\").unwrapped\n",
    "max_action = env.action_space.high[0]\n",
    "min_action = env.action_space.low[0]\n",
    "state_dim = int(env.observation_space.shape[0])\n",
    "action_dim = int(env.action_space.shape[0])\n",
    "# Options: 'Score' trains the policy using the score function and 'Reparam' trains using the reparametrisation trick.\n",
    "training_mode = 'Reparam' \n",
    "number_of_trials = 1\n",
    "save_data = True\n",
    "render_agent = False\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 30\n",
    "num_pretrain_episodes = 1\n",
    "num_training_updates_per_epoch = 200\n",
    "lr = 3e-4\n",
    "discount = 0.99\n",
    "\n",
    "# Initialise data logger\n",
    "results = np.zeros((num_epochs + 1, number_of_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(s):\n",
    "\n",
    "    policy = TanhGaussianPolicy(\n",
    "        max_action=max_action,\n",
    "        min_action=min_action,\n",
    "        input_size=state_dim,\n",
    "        output_size=action_dim\n",
    "    )\n",
    "    policy_optimiser = Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    qf = ValueFunction(\n",
    "        input_size=state_dim + action_dim,\n",
    "    )\n",
    "    qf_optimiser = Adam(qf.parameters(), lr=lr)\n",
    "\n",
    "    vf = ValueFunction(\n",
    "        input_size=state_dim,\n",
    "    )\n",
    "    vf_optimiser = Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "    algorithm = ActorCritic(\n",
    "        policy=policy,\n",
    "        policy_optimiser=policy_optimiser,\n",
    "        qf=qf,\n",
    "        qf_optimiser=qf_optimiser,\n",
    "        vf=vf,\n",
    "        vf_optimiser=vf_optimiser,\n",
    "        env=env,\n",
    "        discount=discount\n",
    "    )\n",
    "\n",
    "    for _ in range(num_pretrain_episodes):\n",
    "        algorithm.sample_episode(exploration_mode=True)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        algorithm.evaluate()\n",
    "        print('Epoch: {}, Average Test Return: {}'.format(e, algorithm.average_return))\n",
    "        results[e, s] = algorithm.average_return\n",
    "        algorithm.reset()\n",
    "\n",
    "        for n in range(num_training_updates_per_epoch):\n",
    "            algorithm.env_step()\n",
    "            if training_mode == 'Score':\n",
    "                algorithm.train_score()\n",
    "            else:\n",
    "                algorithm.train_reparametrisation()\n",
    "\n",
    "\n",
    "    algorithm.evaluate(render=render_agent)\n",
    "    print('Epoch: {}, Average Test Return: {}'.format(e+1, algorithm.average_return))\n",
    "    results[e+1, s] = algorithm.average_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(mean, std):\n",
    "    with open('PartII_{}_{}_trials_{}_epochs.csv'.format(training_mode, number_of_trials, num_epochs), 'w') as resultFile:\n",
    "        wr = csv.writer(resultFile)\n",
    "        mean.insert(0, 'Mean Returns')\n",
    "        std.insert(0, 'Standard Deviation')\n",
    "        data = [mean, std]\n",
    "        data = list(map(list, zip(*data)))\n",
    "        wr.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Test Return: -1436.226726170169\n",
      "Epoch: 1, Average Test Return: -1743.9166432164457\n",
      "Epoch: 2, Average Test Return: -1721.389336417134\n",
      "Epoch: 3, Average Test Return: -1804.0429087041095\n",
      "Epoch: 4, Average Test Return: -1602.0291209324155\n",
      "Epoch: 5, Average Test Return: -1387.342992839241\n",
      "Epoch: 6, Average Test Return: -1138.5623158709604\n",
      "Epoch: 7, Average Test Return: -987.4970038205714\n",
      "Epoch: 8, Average Test Return: -1121.9683403364008\n",
      "Epoch: 9, Average Test Return: -1187.5031595473145\n",
      "Epoch: 10, Average Test Return: -1266.773800495231\n",
      "Epoch: 11, Average Test Return: -1128.5670200566904\n",
      "Epoch: 12, Average Test Return: -1106.5141479711806\n",
      "Epoch: 13, Average Test Return: -1076.5177977888318\n",
      "Epoch: 14, Average Test Return: -688.9558334295328\n",
      "Epoch: 15, Average Test Return: -122.28196776988901\n",
      "Epoch: 16, Average Test Return: -148.97757569028198\n",
      "Epoch: 17, Average Test Return: -199.35968887427063\n",
      "Epoch: 18, Average Test Return: -193.69896690012823\n",
      "Epoch: 19, Average Test Return: -193.25719127953218\n",
      "Epoch: 20, Average Test Return: -209.44854349073412\n",
      "Epoch: 21, Average Test Return: -173.81558474798126\n",
      "Epoch: 22, Average Test Return: -240.5657099414597\n",
      "Epoch: 23, Average Test Return: -167.4094712459814\n",
      "Epoch: 24, Average Test Return: -148.05458386724135\n",
      "Epoch: 25, Average Test Return: -172.30486016823457\n",
      "Epoch: 26, Average Test Return: -146.00184007136914\n",
      "Epoch: 27, Average Test Return: -119.78465523621756\n",
      "Epoch: 28, Average Test Return: -149.0464456668168\n",
      "Epoch: 29, Average Test Return: -198.26896136343095\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for s in range(number_of_trials):\n",
    "        th.manual_seed(s+1)\n",
    "        run_experiment(s)\n",
    "\n",
    "    mean_returns = results.mean(axis=1)\n",
    "    std_returns = results.std(axis=1)\n",
    "    training_steps = np.linspace(0, num_epochs * num_training_updates_per_epoch, num=num_epochs+1)\n",
    "    plt.plot(training_steps, mean_returns)\n",
    "    plt.fill_between(training_steps, mean_returns + std_returns, mean_returns - std_returns, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    if save_data:\n",
    "        write_to_file(mean_returns.tolist(), std_returns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
