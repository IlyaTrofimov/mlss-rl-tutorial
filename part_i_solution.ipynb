{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "PointEnv from rllab\n",
    "The goal is to control an agent and get it to the target located at (0,0).\n",
    "At each timestep the agent gets its current location (x,y) as observation, \n",
    "takes an action (dx,dy), and is transitioned to (x+dx, y+dy).\n",
    "\"\"\"\n",
    "class PointEnv():\n",
    "    def reset(self):\n",
    "        self._state = np.random.uniform(-1, 1, size=(2,))\n",
    "        state = np.copy(self._state)\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, -1, 1)\n",
    "        self._state = self._state + 0.1*action\n",
    "        x, y = self._state\n",
    "        reward = -(x**2 + y**2)**0.5 - 0.02*np.sum(action**2)\n",
    "        done = abs(x) < 0.01 and abs(y) < 0.01\n",
    "        next_state = np.copy(self._state)\n",
    "        return next_state, reward, done\n",
    "\n",
    "\n",
    "class Gauss_Policy():\n",
    "    def __init__(self):\n",
    "        self.action_dim = 2\n",
    "        self.theta = 0.5 * np.ones(4)\n",
    "        # theta here is a length 4 array instead of a matrix for ease of processing\n",
    "        # Think of treating theta as a 2x2 matrix and then flatenning it, which gives us:\n",
    "        # action[0] = state[0]*[theta[0], theta[1]]\n",
    "        # action[1] = state[1]*[theta[2], theta[3]]\n",
    "\n",
    "    def get_action_and_grad(self, state):\n",
    "        # Exercise I.1:\n",
    "        mean_act = np.array([np.dot(self.theta[:2], state), np.dot(self.theta[2:], state)])\n",
    "        sampled_act = mean_act + 0.5 * np.random.randn(self.action_dim)\n",
    "        grad_log_pi = np.ravel([state[0] * (sampled_act - mean_act), state[1] * (sampled_act - mean_act)])\n",
    "        # end\n",
    "        return sampled_act, grad_log_pi\n",
    "\n",
    "# This function collects some trajectories, given a policy\n",
    "def gather_paths(env, policy, num_paths, max_ts=100):\n",
    "    paths = []\n",
    "    for i in range(num_paths):\n",
    "        ts = 0\n",
    "        states = []\n",
    "        act = []\n",
    "        grads = []\n",
    "        rwd = []\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "        while not done and ts<max_ts:\n",
    "            a, grad_a = policy.get_action_and_grad(s)\n",
    "            next_s, r, done = env.step(a)\n",
    "            states += [s]\n",
    "            act += [a]\n",
    "            rwd += [r]\n",
    "            grads += [grad_a]\n",
    "            s = next_s\n",
    "            ts += 1\n",
    "        path = {'states': np.array(states),\n",
    "                'actions': np.array(act),\n",
    "                'grad_log_pi': np.array(grads),\n",
    "                'rwd': np.array(rwd)}\n",
    "        paths += [path]\n",
    "    return paths\n",
    "\n",
    "\n",
    "def baseline(paths):\n",
    "    path_features = []\n",
    "    for path in paths:\n",
    "        s = path[\"states\"]\n",
    "        l = len(path[\"rwd\"])\n",
    "        al = np.arange(l).reshape(-1, 1) / 100.0\n",
    "        path_features += [np.concatenate([s, s ** 2, al, al ** 2, al ** 3, np.ones((l, 1))], axis=1)]\n",
    "    ft = np.concatenate([el for el in path_features])\n",
    "    targets = np.concatenate([el['returns'] for el in paths])\n",
    "\n",
    "    # Exercise I.2(a): Compute the regression coefficents\n",
    "    coeffs = np.linalg.lstsq(ft, targets)[0]\n",
    "    # Exercise I.2(b): Calculate the values for each state\n",
    "    for i, path in enumerate(paths):\n",
    "        path['value'] = np.dot(path_features[i], coeffs)\n",
    "\n",
    "def process_paths(paths, discount_rate=1):\n",
    "    grads = []\n",
    "    for path in paths:\n",
    "        # Exercise 1.3a: Implement the discounted return\n",
    "        path['returns'] = scipy.signal.lfilter([1], [1, float(-discount_rate)], path['rwd'][::-1], axis=0)[::-1]\n",
    "        # End\n",
    "    baseline(paths)\n",
    "    for path in paths:\n",
    "        #path['value'] = np.zeros(len(path['value']))\n",
    "        path['adv'] = path['returns'] - path['value']\n",
    "        rets_for_grads = np.atleast_2d(path['adv']).T\n",
    "        rets_for_grads = np.repeat(rets_for_grads, path['grad_log_pi'].shape[1], axis=1)\n",
    "        path['grads'] = path['grad_log_pi']*rets_for_grads\n",
    "        grads += [np.sum(path['grads'], axis=0)]\n",
    "    grads = np.sum(grads, axis=0)/len(paths)\n",
    "    return grads\n",
    "\n",
    "env = PointEnv()\n",
    "alpha = 0.05\n",
    "num_itr = 1000\n",
    "runs = 2\n",
    "rwd = np.zeros((num_itr, runs))\n",
    "\n",
    "for st in range(runs):\n",
    "    policy = Gauss_Policy()\n",
    "    # print(st)\n",
    "    for i in range(num_itr):\n",
    "        paths = gather_paths(env, policy, num_paths=5)\n",
    "        rwd[i, st] = np.mean([np.sum(path['rwd']) for path in paths])\n",
    "        grads = process_paths(paths, discount_rate=0.995)\n",
    "        policy.theta += alpha * grads\n",
    "\n",
    "mean_rwd = np.mean(rwd, axis=1)\n",
    "sd_rwd = np.std(rwd, axis=1) / np.sqrt(10)\n",
    "plt.plot(mean_rwd)\n",
    "plt.fill_between(np.arange(len(mean_rwd)), mean_rwd + sd_rwd, mean_rwd - sd_rwd, alpha=0.3)\n",
    "plt.ylim([-500, 0])\n",
    "plt.xlim([0, num_itr])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.theta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
