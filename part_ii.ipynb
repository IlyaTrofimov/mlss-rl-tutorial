{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from torch.optim import Adam\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from torch import nn\n",
    "from torch.distributions import Uniform\n",
    "from torch.distributions import Normal\n",
    "from torch.nn import functional as f\n",
    "from copy import deepcopy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pendulum - OpenAI GYM](https://github.com/openai/gym/wiki/Pendulum-v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pendulum problem](https://www.youtube.com/watch?v=1IoN6yCb21s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### storage.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "\n",
    "        self.clear()\n",
    "        self.buffer_size=buffer_size\n",
    "\n",
    "    def add(self,\n",
    "            state,\n",
    "            action,\n",
    "            reward,\n",
    "            next_state,\n",
    "            terminal\n",
    "            ):\n",
    "\n",
    "        if len(self.states) ==  self.buffer_size:\n",
    "            self.clear_earliest_entry()\n",
    "\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "        self.terminals.append(terminal)\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.terminals = []\n",
    "\n",
    "\n",
    "    def clear_earliest_entry(self):\n",
    "\n",
    "        self.states = self.states[1:]\n",
    "        self.actions = self.actions[1:]\n",
    "        self.rewards = self.rewards[1:]\n",
    "        self.next_states = self.next_states[1:]\n",
    "        self.terminals = self.terminals[1:]\n",
    "\n",
    "\n",
    "    def random_batch(self, batch_size=None):\n",
    "\n",
    "        combined = list(zip(self.states,\n",
    "                            self.actions,\n",
    "                            self.rewards,\n",
    "                            self.next_states,\n",
    "                            self.terminals))\n",
    "\n",
    "        random.shuffle(combined)\n",
    "\n",
    "        if batch_size is not None:\n",
    "            combined = combined[:batch_size]\n",
    "\n",
    "        states, actions, rewards, next_states, terminals = zip(*combined)\n",
    "\n",
    "        batch = {}\n",
    "        batch['states'] = th.stack(states)\n",
    "        batch['actions'] = th.stack(actions)\n",
    "        batch['rewards'] = th.stack(rewards)\n",
    "        batch['next_states'] = th.stack(next_states)\n",
    "        batch['terminals'] = th.stack(terminals)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def whole_batch(self,):\n",
    "\n",
    "        batch = {}\n",
    "        batch['states'] = th.stack(self.states)\n",
    "        batch['actions'] = th.stack(self.actions)\n",
    "        batch['rewards'] = th.stack(self.rewards)\n",
    "        batch['next_states'] = th.stack(self.next_states)\n",
    "        batch['terminals'] = th.stack(self.terminals)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### networks.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunction(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "    ):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, activation):\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        activation = self.fc3(activation)\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhGaussianPolicy(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            output_size,\n",
    "            input_size,\n",
    "            max_action,\n",
    "            min_action,\n",
    "            soft_clamp_function=None\n",
    "    ):\n",
    "        super(TanhGaussianPolicy, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.soft_clamp_function = soft_clamp_function\n",
    "        self.max_action = max_action\n",
    "        self.min_action = min_action\n",
    "        self.max_log_sig = 2\n",
    "        self.min_log_sig = -20\n",
    "        self.a_diff = 0.5 * (self.max_action - self.min_action)\n",
    "        self.a_shift = 0.5 * (self.max_action + self.min_action)\n",
    "        self.epsilon = 1e-6\n",
    "\n",
    "        self.fc1 = nn.Linear(self.input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.mu_head = nn.Linear(256, self.output_size)\n",
    "        self.log_sig_head = nn.Linear(256, self.output_size)\n",
    "\n",
    "    def forward(self, activation):\n",
    "\n",
    "        activation = f.relu(self.fc1(activation))\n",
    "        activation = f.relu(self.fc2(activation))\n",
    "        mu = self.mu_head(activation)\n",
    "        log_sig = self.log_sig_head(activation)\n",
    "        log_sig = log_sig.clamp(min=self.min_log_sig, max=self.max_log_sig)\n",
    "        sig = th.exp(log_sig)\n",
    "\n",
    "        return mu, sig\n",
    "\n",
    "    def tanh_function(self, a):\n",
    "\n",
    "        a = self.a_diff * th.tanh(a / self.a_diff) + self.a_shift\n",
    "\n",
    "        return a\n",
    "\n",
    "    def tanh_function_derivative(self, a):\n",
    "\n",
    "        return 1 - (th.tanh(a / self.a_diff) ** 2) + self.epsilon\n",
    "\n",
    "    def get_action(self, state, eval_deterministic=False):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        if eval_deterministic:\n",
    "            action = self.tanh_function(mu)\n",
    "        else:\n",
    "            gauss = Normal(loc=mu, scale=sig)\n",
    "            action = gauss.sample()\n",
    "            action = self.tanh_function(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_action_and_log_prob(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        pre_tanh_action = gauss.sample()\n",
    "        pre_tanh_log_prob = gauss.log_prob(pre_tanh_action)\n",
    "        action = self.tanh_function(pre_tanh_action)\n",
    "        log_prob = pre_tanh_log_prob - th.log(self.tanh_function_derivative(pre_tanh_action))\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def r_sample(self, state):\n",
    "\n",
    "        mu, sig = self.forward(state)\n",
    "        loc = th.zeros(size=[state.shape[0], 1], dtype=th.float32)\n",
    "        scale = loc + 1.0\n",
    "        unit_gauss = Normal(loc=loc, scale=scale)\n",
    "        epsilon = unit_gauss.sample()\n",
    "        pre_tanh_action = mu + sig * epsilon\n",
    "        action = self.tanh_function(pre_tanh_action)\n",
    "\n",
    "        gauss = Normal(loc=mu, scale=sig)\n",
    "        pre_tanh_log_prob = gauss.log_prob(pre_tanh_action)\n",
    "        log_prob = pre_tanh_log_prob - th.log(self.tanh_function_derivative(pre_tanh_action))\n",
    "\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the policy gradient theorem for continuous domains. Recall the reinforcement learning objective, which we write here in full:\n",
    "\\begin{align}\n",
    "J(\\theta)=\\int p_0(s)\\int \\pi_\\theta(a\\vert s) Q(a,s) da ds. \\label{eq:rl_objective}\n",
    "\\end{align}\n",
    "\n",
    "+ Actor-Critic : Learn Q-function\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta)=\\int p_0(s) \\int \\pi_\\theta(a\\vert s) Q_{\\omega}(a, s) dads.\n",
    "\\end{align}\n",
    "\n",
    "+ Add baseline (Value Function)\n",
    "\\begin{align}\n",
    "A_{\\omega}(a, s) = Q_{\\omega}(a, s) - V_{\\omega}(s)\n",
    "\\end{align}\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)= \\nabla_\\theta \\int p_0(s) \\int \\pi_\\theta(a\\vert s) A_{\\omega}(a, s)  dads.\n",
    "\\end{align}\n",
    "\n",
    "+ Add more exploration\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)= \\nabla_\\theta \\int p_0(s) \\int \\pi_\\theta(a\\vert s) \\left(A_{\\omega}(a, s) -\\log\\pi_\\theta(a\\vert s)\\right) dads.\n",
    "\\end{align}\n",
    "\n",
    "II.2a) Why does the additional entropy term in \\cref{eq:entropy_objective} encourages exploration? Hint: What happens when the policy is very confident about a particular action? What about when all actions have equal probability?\n",
    "Taking derivatives of \\cref{eq:entropy_objective} using the policy gradient theorem with the log-derivative trick then yields the gradient update:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)=&\\int \\rho^\\pi(s)\\nabla_\\theta \\int \\pi_\\theta(a\\vert s) \\left(A_{\\omega}(a, s) -\\log\\pi_\\theta(a\\vert s)\\right)  dads, \\\\\n",
    "=&\\mathbb{E}_{s\\sim\\rho^\\pi(s), a\\sim\\pi_\\theta(a\\vert s)}\\Big[\\nabla_\\theta\\Big(\\log\\pi_\\theta(a\\vert s) \\left(A_{\\omega}(a, s)-\\log\\pi_\\theta(a\\vert s)\\right)\\Big)\\Big]. \\label{eq:entropy_expectation}\n",
    "\\end{align}\n",
    "II.2b) Implement the score function gradient update with entropy as derived in \\cref{eq:entropy_expectation} in the function \\code{train\\_score()}, uncommenting line 173 to allow gradient updates to the policy. If it is implemented correctly, you should start to see the average test return rise above -1000 for a least one epoch by 25 epochs of training. If the agent is training, remove the entropy term and test for 1 trial to show that it doesn't learn to improve it's policy within 30 epochs. Then let it run for the full 5 trials with the entropy term and move on to the next part while it completes (it should take around 20 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(\n",
    "            self,\n",
    "            policy,\n",
    "            qf,\n",
    "            env,\n",
    "            discount,\n",
    "            qf_optimiser,\n",
    "            policy_optimiser,\n",
    "            max_evaluation_episode_length=200,\n",
    "            num_evaluation_episodes=5,\n",
    "            num_training_episode_steps=1000,\n",
    "            batch_size=128,\n",
    "            buffer_size = 10000,\n",
    "            eval_deterministic = True,\n",
    "            training_on_policy = False,\n",
    "            vf=None,\n",
    "            vf_optimiser=None\n",
    "    ):\n",
    "\n",
    "        self.policy = policy\n",
    "        self.qf = qf\n",
    "        self.vf = vf\n",
    "        self.target_vf = deepcopy(vf)\n",
    "        self.tau = 1e-2\n",
    "        self.vf_optimiser = vf_optimiser\n",
    "        self.qf_optimiser = qf_optimiser\n",
    "        self.policy_optimiser = policy_optimiser\n",
    "        self.env = env\n",
    "        self.discount = discount\n",
    "        self.batch_size = batch_size\n",
    "        self.max_evaluation_episode_length = max_evaluation_episode_length\n",
    "        self.num_evaluation_episodes = num_evaluation_episodes\n",
    "        self.num_training_episode_steps = num_training_episode_steps\n",
    "        self.training_on_policy = training_on_policy\n",
    "        self.buffer = Buffer(buffer_size=buffer_size)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.pretraining_policy = Uniform(high=th.Tensor([policy.max_action]), low=th.Tensor([policy.min_action]))\n",
    "        self.eval_deterministic = eval_deterministic\n",
    "\n",
    "        self.R_av = None\n",
    "        self.R_tot = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = th.from_numpy(self.env.reset()).float()\n",
    "\n",
    "    def evaluate(self, render=False):\n",
    "\n",
    "        total_return = 0\n",
    "\n",
    "        for _ in range(self.num_evaluation_episodes):\n",
    "            state = th.from_numpy(self.env.reset()).float()\n",
    "            episode_return = 0\n",
    "\n",
    "            for _ in range(self.max_evaluation_episode_length):\n",
    "                action = self.policy.get_action(state, self.eval_deterministic)\n",
    "                action = np.array([action.item()])\n",
    "\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                state, reward, terminal, _ = self.env.step(action)\n",
    "                state = th.from_numpy(state).float()\n",
    "\n",
    "                episode_return += reward\n",
    "\n",
    "                if terminal:\n",
    "                    break\n",
    "\n",
    "            total_return += episode_return\n",
    "\n",
    "        self.average_return = total_return/self.num_evaluation_episodes\n",
    "\n",
    "    def sample_episode(self, exploration_mode=False):\n",
    "\n",
    "        self.reset()\n",
    "        state = self.state\n",
    "\n",
    "        for _ in range(self.num_training_episode_steps):\n",
    "\n",
    "            if exploration_mode:\n",
    "                action = self.pretraining_policy.sample()\n",
    "            else:\n",
    "                action = self.policy.get_action(state)\n",
    "            next_state, reward, terminal, _ = self.env.step(action.numpy())\n",
    "            next_state = th.from_numpy(next_state).float()\n",
    "            reward = th.Tensor([reward])\n",
    "            terminal = th.Tensor([terminal])\n",
    "\n",
    "            self.buffer.add(state=state,\n",
    "                            action=action,\n",
    "                            reward=reward,\n",
    "                            next_state=next_state,\n",
    "                            terminal=terminal)\n",
    "\n",
    "            state = next_state\n",
    "            if terminal:\n",
    "                self.reset()\n",
    "                state = self.state\n",
    "\n",
    "    def env_step(self):\n",
    "\n",
    "        state = self.state\n",
    "        action = self.policy.get_action(state)\n",
    "        next_state, reward, terminal, _ = self.env.step(action.numpy())\n",
    "        next_state = th.from_numpy(next_state).float()\n",
    "        reward = th.Tensor([reward])\n",
    "        terminal = th.Tensor([terminal])\n",
    "\n",
    "        self.buffer.add(state=state,\n",
    "                        action=action,\n",
    "                        reward=reward,\n",
    "                        next_state=next_state,\n",
    "                        terminal=terminal)\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "    def train_score(self):\n",
    "        if self.training_on_policy:\n",
    "            batch = self.buffer.whole_batch()\n",
    "            self.buffer.clear()\n",
    "        else:\n",
    "            batch = self.buffer.random_batch(self.batch_size)\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        next_states = batch['next_states']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        new_actions, log_pis = self.policy.get_action_and_log_prob(states)\n",
    "        values = self.vf(states)\n",
    "        state_actions = th.cat((states, actions), 1)\n",
    "        q_values = self.qf(state_actions)\n",
    "        next_values = self.target_vf(next_states)\n",
    "        new_state_actions = th.cat((states, new_actions), 1)\n",
    "        new_q_values = self.qf(new_state_actions)\n",
    "\n",
    "        \"\"\"\n",
    "        Value (Critic) Losses:\n",
    "        \"\"\"\n",
    "\n",
    "        v_targets = new_q_values\n",
    "        vf_loss = (v_targets.detach() - values).pow(2).mean()\n",
    "\n",
    "        q_targets = rewards + self.discount * (1 - terminals) * next_values\n",
    "        qf_loss = (q_targets.detach() - q_values).pow(2).mean()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Policy (Actor) Losses: TO COMPLETE IN EXERCISE II.2b\n",
    "        \"\"\"\n",
    "        adv = ...\n",
    "        policy_loss = ...\n",
    "        \n",
    "        \"\"\"\n",
    "        Gradient Updates\n",
    "        \"\"\"\n",
    "        self.qf_optimiser.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimiser.step()\n",
    "\n",
    "        self.vf_optimiser.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimiser.step()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "    def train_reparametrisation(self):\n",
    "\n",
    "        if self.training_on_policy:\n",
    "            batch = self.buffer.whole_batch()\n",
    "            self.buffer.clear()\n",
    "        else:\n",
    "            batch = self.buffer.random_batch(self.batch_size)\n",
    "        states = batch['states']\n",
    "        actions = batch['actions']\n",
    "        rewards = batch['rewards']\n",
    "        next_states = batch['next_states']\n",
    "        terminals = batch['terminals']\n",
    "\n",
    "        state_actions = th.cat((states, actions), 1)\n",
    "        q_pred = self.qf(state_actions)\n",
    "        v_pred = self.vf(states)\n",
    "        new_actions, log_pis = self.policy.r_sample(states)\n",
    "\n",
    "        \"\"\"\n",
    "        Value (Critic) Losses:\n",
    "        \"\"\"\n",
    "        target_v_values = self.target_vf(next_states)\n",
    "        q_target = rewards + (1. - terminals) * self.discount * target_v_values\n",
    "        qf_loss = self.loss(q_pred, q_target.detach())\n",
    "\n",
    "        new_state_actions = th.cat((states, new_actions), 1)\n",
    "        q_new_actions = self.qf(new_state_actions)\n",
    "        v_target = q_new_actions\n",
    "        vf_loss = self.loss(v_pred, v_target.detach())\n",
    "\n",
    "        \"\"\"\n",
    "        Policy (Actor) Loss: TO COMPLETE IN EXERCISE II.3c     \n",
    "        \"\"\"\n",
    "        policy_loss = ...\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Gradient Updates\n",
    "        \"\"\"\n",
    "        self.qf_optimiser.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        self.qf_optimiser.step()\n",
    "\n",
    "        self.vf_optimiser.zero_grad()\n",
    "        vf_loss.backward()\n",
    "        self.vf_optimiser.step()\n",
    "\n",
    "        self.policy_optimiser.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimiser.step()\n",
    "\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        for target_param, param in zip(self.target_vf.parameters(), self.vf.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - self.tau) + param.data * self.tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise II.3: Reducing Variance using the Reparametrisation Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators like the score function estimator from II.1 often have very high variance for their gradient updates, even when a baseline is used. We now see how performance can be further improved by using the reparametrisation trick to exploit the low variance properties of using an update with the first order derivative $\\nabla_aQ_\\omega(a,s)$. The results are analogous to those derived for variational auto-encoders (\\href{https://arxiv.org/pdf/1312.6114.pdf}{Kingma and Welling, 2013})\n",
    "\n",
    "Instead of sampling an action directly from our policy $a\\sim \\pi_\\theta(a\\vert s)$, we sample a variable $\\epsilon$ from a zero-mean Gaussian distribution with standard deviation of unity $\\epsilon\\sim p(\\epsilon)=\\mathcal{N}(0,1)$. An action is then constructed using $a_\\theta=\\tanh{(\\epsilon\\sigma_\\theta + \\mu_\\theta)}$. Although not strictly justified in the case of the classic policy gradient theorem, as we have used function approximators for the value functions, to aid our analysis we can shift the derivative outside of the inner integral:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)=\\int\\rho^\\pi(s)\\nabla_\\theta\\int\\pi_\\theta(a\\vert s) (A_\\omega(a,s)-\\log\\pi_\\theta(a\\vert s)\\pi_\\theta(a\\vert s)) da ds \\label{eq:shifted_derivative}\n",
    "\\end{align}\n",
    "\n",
    "II.3a) Using the substitution $a=a_\\theta=\\tanh{(\\epsilon\\sigma_\\theta + \\mu_\\theta)}$, show that derivative can be written as\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta)=\\mathbb{E}_{s\\sim\\rho^\\pi(s), \\epsilon\\sim  p(\\epsilon)}\\Big[\\nabla_\\theta(A_\\omega(a_\\theta ,s)-\\log\\pi_\\theta(a_\\theta\\vert s))\\Big], \\label{eq:reparam}\n",
    "\\end{align}\n",
    "II.3b) Implement the reparametrisation gradient update with entropy as derived in \\cref{eq:reparam2} in the function \\code{train\\_reparametrisation()}. If implemented correctly, you should start to see the average test return rise above -1000 at an earlier epoch than when the score function estimator is used. Compare the performance of your new plot against the score function plot from II.1b by running the file 'plot\\_all.py'.\n",
    "\n",
    "Hints:\n",
    "* Don't forget to change \"training\\_mode\" to \"reparam\" before running.\n",
    "* \"new\\_actions\" and \"log\\_pis\" have now been calculated for you using the function \"r\\_sample\", sampling $\\epsilon$ first from $p(\\epsilon)$ and then calculating $a_\\theta=\\tanh{(\\epsilon\\sigma_\\theta + \\mu_\\theta)}$. This means that they have dependency on policy parameters $\\theta$.\n",
    "* \"q\\_new\\_actions\" now also have dependency on policy parameters $\\theta$, so gradients will automatically flow through these q-values back to the network parameters when \"backward()\" is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Environment\n",
    "env = gym.make(\"Pendulum-v0\").unwrapped\n",
    "max_action = env.action_space.high[0]\n",
    "min_action = env.action_space.low[0]\n",
    "state_dim = int(env.observation_space.shape[0])\n",
    "action_dim = int(env.action_space.shape[0])\n",
    "training_mode = 'Reparam' # Options: 'Score' trains the policy using the score function and 'Reparam' trains using the reparametrisation trick.\n",
    "number_of_trials = 1\n",
    "save_data = True\n",
    "render_agent = True\n",
    "\n",
    "# Set parameters\n",
    "num_epochs = 30\n",
    "num_pretrain_episodes = 1\n",
    "num_training_updates_per_epoch = 200\n",
    "lr = 3e-4\n",
    "discount = 0.99\n",
    "\n",
    "# Initialise data logger\n",
    "results = np.zeros((num_epochs + 1, number_of_trials))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(s):\n",
    "\n",
    "    policy = TanhGaussianPolicy(\n",
    "        max_action=max_action,\n",
    "        min_action=min_action,\n",
    "        input_size=state_dim,\n",
    "        output_size=action_dim\n",
    "    )\n",
    "    policy_optimiser = Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    qf = ValueFunction(\n",
    "        input_size=state_dim + action_dim,\n",
    "    )\n",
    "    qf_optimiser = Adam(qf.parameters(), lr=lr)\n",
    "\n",
    "    vf = ValueFunction(\n",
    "        input_size=state_dim,\n",
    "    )\n",
    "    vf_optimiser = Adam(vf.parameters(), lr=lr)\n",
    "\n",
    "    algorithm = ActorCritic(\n",
    "        policy=policy,\n",
    "        policy_optimiser=policy_optimiser,\n",
    "        qf=qf,\n",
    "        qf_optimiser=qf_optimiser,\n",
    "        vf=vf,\n",
    "        vf_optimiser=vf_optimiser,\n",
    "        env=env,\n",
    "        discount=discount\n",
    "    )\n",
    "\n",
    "    for _ in range(num_pretrain_episodes):\n",
    "        algorithm.sample_episode(exploration_mode=True)\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        algorithm.evaluate()\n",
    "        print('Epoch: {}, Average Test Return: {}'.format(e, algorithm.average_return))\n",
    "        results[e, s] = algorithm.average_return\n",
    "        algorithm.reset()\n",
    "\n",
    "        for n in range(num_training_updates_per_epoch):\n",
    "            algorithm.env_step()\n",
    "            if training_mode == 'Score':\n",
    "                algorithm.train_score()\n",
    "            else:\n",
    "                algorithm.train_reparametrisation()\n",
    "\n",
    "\n",
    "    algorithm.evaluate(render=render_agent)\n",
    "    print('Epoch: {}, Average Test Return: {}'.format(e+1, algorithm.average_return))\n",
    "    results[e+1, s] = algorithm.average_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(mean, std):\n",
    "    with open('PartII_{}_{}_trials_{}_epochs.csv'.format(training_mode, number_of_trials, num_epochs), 'w') as resultFile:\n",
    "        wr = csv.writer(resultFile)\n",
    "        mean.insert(0, 'Mean Returns')\n",
    "        std.insert(0, 'Standard Deviation')\n",
    "        data = [mean, std]\n",
    "        data = list(map(list, zip(*data)))\n",
    "        wr.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Test Return: -1298.0750331607965\n",
      "Epoch: 1, Average Test Return: -1551.9279468850723\n",
      "Epoch: 2, Average Test Return: -1653.2247266382142\n",
      "Epoch: 3, Average Test Return: -1810.696097423105\n",
      "Epoch: 4, Average Test Return: -1626.9534585466158\n",
      "Epoch: 5, Average Test Return: -1384.7575452946585\n",
      "Epoch: 6, Average Test Return: -1185.5564738112578\n",
      "Epoch: 7, Average Test Return: -760.9102925011953\n",
      "Epoch: 8, Average Test Return: -1050.401373053096\n",
      "Epoch: 9, Average Test Return: -857.7493298290943\n",
      "Epoch: 10, Average Test Return: -1026.7957731260724\n",
      "Epoch: 11, Average Test Return: -407.65012654055465\n",
      "Epoch: 12, Average Test Return: -401.12601501593343\n",
      "Epoch: 13, Average Test Return: -148.31344697511423\n",
      "Epoch: 14, Average Test Return: -237.16160710353762\n",
      "Epoch: 15, Average Test Return: -123.70537844690507\n",
      "Epoch: 16, Average Test Return: -237.48314413793673\n",
      "Epoch: 17, Average Test Return: -176.25311373855538\n",
      "Epoch: 18, Average Test Return: -170.2184912121025\n",
      "Epoch: 19, Average Test Return: -193.74408300418082\n",
      "Epoch: 20, Average Test Return: -98.79306159538578\n",
      "Epoch: 21, Average Test Return: -142.23194695297548\n",
      "Epoch: 22, Average Test Return: -166.27266835989047\n",
      "Epoch: 23, Average Test Return: -185.71033976855165\n",
      "Epoch: 24, Average Test Return: -227.62421912947306\n",
      "Epoch: 25, Average Test Return: -146.0354632902136\n",
      "Epoch: 26, Average Test Return: -142.95615149435827\n",
      "Epoch: 27, Average Test Return: -143.6798250028732\n",
      "Epoch: 28, Average Test Return: -186.6867875840779\n",
      "Epoch: 29, Average Test Return: -169.79465395559978\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-5169d8ebc12f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmean_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-5b7e8de85f9d>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_agent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}, Average Test Return: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-a31cee2da66a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, render)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for s in range(number_of_trials):\n",
    "        th.manual_seed(s+1)\n",
    "        run_experiment(s)\n",
    "\n",
    "    mean_returns = results.mean(axis=1)\n",
    "    std_returns = results.std(axis=1)\n",
    "    training_steps = np.linspace(0, num_epochs * num_training_updates_per_epoch, num=num_epochs+1)\n",
    "    plt.plot(training_steps, mean_returns)\n",
    "    plt.fill_between(training_steps, mean_returns + std_returns, mean_returns - std_returns, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    if save_data:\n",
    "        write_to_file(mean_returns.tolist(), std_returns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
