{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> DAY 2: Approximate Reinforcement Learning </h1>\n",
    "<p> <img src=\"http://skyai.org/wiki/?plugin=attach&refer=Documentation%2FTutorial%20-%20Example%20-%20Mountain%20Car&openfile=mountaincar.png\" alt=\"picture of the mountain-car environment\" style=\"float:right;width:500px;border:10px solid #FFFFFF;\">\n",
    "<h3> MountainCar </h3>\n",
    "<p> A car starts in a valley between two mountains, as depicted in the image to the right. The car must reach the goal location on the top of the right mountain by using three possible actions: accelerate forwards, backwards or doing nothing. However, the car's motor is underactuated and cannot drive straight uphill. The agent's task is to find a policy that swings the car back and forth such that it eventually reaches the goal position. </p>\n",
    "<p> The state space contains the continuous position of the car in [-1.2, 0.6] and its continuous velocity in [-0.07, 0.07]. The reward is 0 for reaching the goal and -1 for each other step. Episodes end after reaching the goal or 1000 steps otherwise (this is a deviation from the classical task). </p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 1: Tabular Q-learning\n",
    "In this exercise, you will familiarise yourself with a given agent class and an experiment script that runs OpenAI Gym environments.\n",
    "\n",
    "In the following code block, we define a longer Mountain-Car environment from the OpenAI Gym.\n",
    "Note that some installed libraries may throw some warnings here. Ignore them unless task 1c does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "import random\n",
    "from math import e as nate\n",
    "\n",
    "def make_environment():\n",
    "    \"\"\" Returns a new mountain-car environment. \"\"\"\n",
    "    # Register special version of MountainCar with long episodes\n",
    "    gym.envs.register(\n",
    "        id='MountainCarExtraLong-v0',\n",
    "        entry_point='gym.envs.classic_control:MountainCarEnv',\n",
    "        max_episode_steps=1000,\n",
    "        reward_threshold=-110.0,\n",
    "    )\n",
    "    # Create environment\n",
    "    env = gym.make('MountainCarExtraLong-v0')\n",
    "    env_name = 'MountainCarExtraLong-v0'\n",
    "    env = gym.make(env_name)\n",
    "    # Set seeds (for reproduceability)\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    return env\n",
    "\n",
    "# Make a mountain-car environment\n",
    "env = make_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1a: the agent \n",
    "Familiarise yourself with the below specification of a QLearner at the example of the following Tabular Q-Learner. \n",
    "Given the action to execute, gym environments return the next state, the received reward and a Boolean done, which indicated that the episode has ended.\n",
    "\n",
    "Complete the update() function of the TabularQLearner by filling in the TD-error for Q-learning. Make sure that the value of the next state is 0 for the last step of an episode, that is, when done=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    \"\"\" Specifies a RL agent that can compute Q-values. \"\"\"\n",
    "    gamma = 0.9    # discount factor\n",
    "    name = None    # the name of this agent (for the legend in plots)\n",
    "    epsilon = 0.1  # the exploration parameter for epsilon-greedy\n",
    "    \n",
    "    def q_values(self, state):\n",
    "        \"\"\" Returns the estimated Q-values (as a np.ndarray) of the given state. \"\"\"\n",
    "        assert False, \"Abstract class must be inherited from to get Q-values.\"\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\" Returns a greedily sampled action according to the estimated Q-values of the given state. \"\"\"\n",
    "        assert False, \"Abstract class must be inherited from to sample.\"\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Updates the Q-value estimate after observing a transition from 'state', using 'action',\n",
    "            receiving 'reward' and ending up in 'next_state'. The Boolean 'done' indicates whether\n",
    "            or not the episode has ended with this transition. Returns nothing. \"\"\"\n",
    "        assert False, \"Abstract class must be inherited from to update.\"\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        \"\"\" Returns the exploration parameter of epsilon-greedy. \"\"\"\n",
    "        return self.epsilon\n",
    "    \n",
    "    def set_epsilon(self, iter):\n",
    "        \"\"\" Can be overwritten to change the exploration parameter during training. \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TabularQLearner (QLearner):\n",
    "    \"\"\" Tabular Q-learning agent. \"\"\"\n",
    "    learn_rate = 0.1    # learning rate of the Q-learning update\n",
    "    n_states = 40       # number of states per state-dimension\n",
    "    q_table = None      # table with Q-values\n",
    "    env_low = None\n",
    "    env_dx = None\n",
    "\n",
    "    def __init__(self, env, n_states=None):\n",
    "        \"\"\" Creates a tabular Q-learning agent with n_states in each of teh env(ironments) state directions. \"\"\"\n",
    "        self.name = \"Tabular (%u states)\" % (n_states ** 2)\n",
    "        if n_states is not None:\n",
    "            self.n_states = n_states\n",
    "        self.env_low = env.observation_space.low\n",
    "        self.env_dx = (env.observation_space.high - self.env_low) / (self.n_states - 1)\n",
    "        self.q_table = np.zeros((self.n_states, self.n_states, env.action_space.n))\n",
    "\n",
    "    def _state_to_index(self, obs):\n",
    "        \"\"\" Maps an observed state to an index of the q_table. \"\"\"\n",
    "        a = int((obs[0] - self.env_low[0]) / self.env_dx[0])\n",
    "        b = int((obs[1] - self.env_low[1]) / self.env_dx[1])\n",
    "        return a, b\n",
    "\n",
    "    def q_values(self, state):\n",
    "        \"\"\" Returns the estimated Q-values (as a np.ndarray) of the given state. \"\"\"\n",
    "        a, b = self._state_to_index(state)\n",
    "        return self.q_table[a][b]\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\" Returns a greedily sampled action according to the estimated Q-values of the given state. \"\"\"\n",
    "        return np.argmax(self.q_values(state))\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" The agent 'learns' from the given transition. \"\"\"\n",
    "        not_done = 0 if done else 1\n",
    "        a, b = self._state_to_index(state)\n",
    "        a_, b_ = self._state_to_index(next_state)\n",
    "        # Update Q-table with the TD-error for Q-learning (fill in to complete class)\n",
    "        td_error = ...\n",
    "        self.q_table[a][b][action] = self.q_table[a][b][action] + self.learn_rate * td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code-block contains a plotting function that keeps track of the experimental results. Re-executing this block, or calling clear_plots(), will clear all previous experiments from future plots; otherwise the specifics of the code block can be ignored (but has to be exectuted) for the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_plots():\n",
    "    \"\"\" Clears the plot buffers from previous experimental results\"\"\"\n",
    "    global last_agent, plot_rewards, plot_labels\n",
    "    last_agent = None\n",
    "    plot_rewards = []\n",
    "    plot_labels = []\n",
    "\n",
    "def compute_q_values(env):\n",
    "    \"\"\" Computes the Q-values on an equidistant grid. \"\"\"\n",
    "    dim = 100    # resolution of the resulting image\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    # Create the states\n",
    "    states = np.zeros((2, dim ** 2))\n",
    "    states[0, :] = np.tile(np.linspace(env_low[0], env_high[0], num=dim), dim)\n",
    "    states[1, :] = np.repeat(np.linspace(env_high[1], env_low[1], num=dim), dim)\n",
    "    q_values = np.zeros((3, dim ** 2))\n",
    "    for i in range(dim ** 2):\n",
    "        q = last_agent.q_values(states[:, i])\n",
    "        q_values[:, i] = q.detach().cpu().numpy() if isinstance(q, torch.Tensor) else q\n",
    "    return q_values\n",
    "\n",
    "def compute_trajectory(env, agent):\n",
    "    \"\"\" Returns a trajectory (as np.ndarray) in the given env(ironment) executed by the given agent. \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    state = env.reset()\n",
    "    traj = [(state - env_low) / (env_high - env_low)]\n",
    "    done = False\n",
    "    while(not done):\n",
    "        action = agent.sample(state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "        traj.append((state - env_low) / (env_high - env_low))\n",
    "    return np.stack(traj, axis=1)\n",
    "\n",
    "def plot_as_image(ax, env, values):\n",
    "    \"\"\" Plots a given np.ndarray of values as a sqare image in the given ax(es). \"\"\"\n",
    "    dim = int(np.sqrt(len(values)))\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    cax = ax.imshow(values.reshape((dim, dim)), extent=[0, 1, 0, 1], cmap='jet')\n",
    "    ax.set_xlabel('Car Position')\n",
    "    ax.set_xticklabels([\"%g\" % (env_low[0] + i * (env_high[0] - env_low[0]) / 5) for i in range(6)])\n",
    "    ax.set_ylabel('Car Velocity')\n",
    "    ax.set_yticklabels([\"%g\" % (env_low[1] + i * (env_high[1] - env_low[1]) / 5) for i in range(6)])\n",
    "    return plt.gcf().colorbar(cax, ax=ax)\n",
    "\n",
    "def plot_all_results(env, plot_std=True):\n",
    "    \"\"\" Plots the performance of all experiments in the result-buffers, as well as the value function/policy \n",
    "        of the agent at the end of the last experiment. \"\"\"\n",
    "    colors = ['orange', 'red', 'magenta', 'blue', 'green', 'black', 'c', 'y', 'lime']\n",
    "    # Generate figure and subplot grid\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 5)\n",
    "    ax1 = plt.subplot(gs[:, :3])\n",
    "    ax2 = plt.subplot(gs[0, 3:])\n",
    "    ax3 = plt.subplot(gs[1, 3:])\n",
    "    plt.gcf().set_size_inches([16, 7.5])\n",
    "    # Plot the performance\n",
    "    if plot_std:\n",
    "        # Make a nice plot with mean and standard deviation for every 10 samples\n",
    "        for i in range(len(plot_rewards)):\n",
    "            rew = np.array(plot_rewards[i])\n",
    "            rew = rew[:(len(rew) - len(rew) % 10)].reshape(int(len(plot_rewards[i]) / 10), 10)\n",
    "            m = np.mean(rew, axis=1)\n",
    "            s = np.std(rew, axis=1)\n",
    "            x = np.linspace(5, 10 * len(m) + 5, len(m))\n",
    "            ax1.fill_between(x=x, y1=m - s, y2=m + s, alpha=0.2, linewidth=0, facecolor=colors[i % len(colors)])\n",
    "            ax1.plot(x, m, color=colors[i % len(colors)])\n",
    "        ax1.set_xlabel(\"Environmental Steps\")\n",
    "        ax1.set_ylabel(\"Episode Reward (STD)\")\n",
    "        ax1.set_ylim(-1000, 0)\n",
    "        ax1.set_xlim(0, len(plot_rewards[0]))\n",
    "    else:\n",
    "        # Use an ugly plot that plots every single measurement\n",
    "        for rewards in plot_rewards:\n",
    "            ax1.plot(rewards)\n",
    "        ax1.set_xlabel(\"Epsiodes\")\n",
    "        ax1.set_ylabel(\"Episode Reward\")\n",
    "    ax1.legend(plot_labels, loc='lower right')\n",
    "    # Generate Q-values and example trajectories\n",
    "    traj_list = [compute_trajectory(env, last_agent) for _ in range(3)]\n",
    "    colors = ['darkgrey', 'lightgrey', 'white']\n",
    "    q_values = compute_q_values(env)\n",
    "    # Plot the value function\n",
    "    plot_as_image(ax2, env, q_values.max(axis=0))\n",
    "    for i, traj in enumerate(traj_list):\n",
    "        ax2.plot(traj[0, :], traj[1, :], color=colors[i % len(colors)])\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_title('Value Function & Greedy Policy')\n",
    "    # Plot the policy\n",
    "    cbar = plot_as_image(ax3, env, q_values.argmax(axis=0))\n",
    "    for traj in traj_list:\n",
    "         ax3.plot(traj[0, :], traj[1, :], color=colors[i % len(colors)])\n",
    "    cbar.set_ticks([0, 1, 2])\n",
    "    cbar.ax.set_yticklabels(['-acc', '0', '+acc'])\n",
    "    plt.show()\n",
    "    \n",
    "clear_plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1b: the experiment \n",
    "Familiarize yourself with the basic loop of the RL experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent: QLearner):\n",
    "    iter_max = 1000     # number of episodes\n",
    "    # Set seeds (for reproduceability)\n",
    "    env.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    # Initialise the result lists\n",
    "    total_reward_list = []\n",
    "    total_steps = 0\n",
    "    print('----- Start Learning with %s Q-learning -----' % agent.name)\n",
    "    # For iter_max episodes\n",
    "    for iter in range(iter_max):\n",
    "        # Reset the episode\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        # Run the episode until it is done (as signaled by the environment)\n",
    "        while not done:\n",
    "            current_state = state\n",
    "            # Epsilon greedy action selection\n",
    "            if np.random.uniform(0, 1) < agent.get_epsilon():\n",
    "                # Random choice\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                # Greedy choice\n",
    "                action = agent.sample(current_state)\n",
    "            # One environmental step\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            # Update agent\n",
    "            agent.update(current_state, action, reward, state, done)\n",
    "            total_steps += 1\n",
    "        agent.set_epsilon(iter)\n",
    "        total_reward_list.append(total_reward)\n",
    "        # Output for user (every 100 episodes)\n",
    "        if iter % 100 == 49:\n",
    "            print('Episode #%d (%u steps) -- Total reward = %g, epsilon=%g.' % \n",
    "                  (iter+1, total_steps, total_reward, agent.get_epsilon()))\n",
    "    # Book keeping for the plotting script\n",
    "    if agent.name in plot_labels:\n",
    "        plot_rewards[plot_labels.index(agent.name)] = total_reward_list\n",
    "    else:\n",
    "        plot_labels.append(agent.name)\n",
    "        plot_rewards.append(total_reward_list)\n",
    "    global last_agent\n",
    "    last_agent = agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1c: test the tabular agent \n",
    "Run the above experiment with a TabularQLearner, which uses a discretization of 40 states per dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, TabularQLearner(env, 40))\n",
    "plot_all_results(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the resulting performance, value function and policy with your neighbours. Do you see a consistent behaviour policy, and if not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2: Approximate Q-learning\n",
    "In this exercise you will implement a QLearner based on gradient descend with a linear Q-value function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2a: basis functions\n",
    "Now we want to use approximate Q-learning for linear functions. Familiarize yourself with our specification of basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasisFunctions:\n",
    "    \"\"\" Abstract class that specifies basis functions. \"\"\"\n",
    "    name = None                  # the name of the basis function\n",
    "    num_features = None          # the number of basis functions\n",
    "    _env_low = None\n",
    "    _env_high = None\n",
    "    _env_dx = None\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env_low = env.observation_space.low\n",
    "        self._env_high = env.observation_space.high\n",
    "        self._env_dx = self._env_high - self._env_low\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\" Returns the basis function outputs of the given state as a vector.  \"\"\"\n",
    "        assert False, \"Abstract class must be inherited from to call.\"\n",
    "\n",
    "    def new_weights(self):\n",
    "        \"\"\" Returns a newly initialized fitting weight vector. \"\"\"\n",
    "        return np.zeros(self.num_features)\n",
    "\n",
    "class OneHot (BasisFunctions):\n",
    "    \"\"\" one-hot encoding of 2d-spaces \"\"\"\n",
    "    n_states = None\n",
    "\n",
    "    def __init__(self, env, n_states):\n",
    "        \"\"\" Creates a one-hot encoding with n_states discrete intervals in each input dimension. \"\"\"\n",
    "        BasisFunctions.__init__(self, env)\n",
    "        self.name = \"OneHot\"\n",
    "        self.n_states = n_states\n",
    "        self.num_features = n_states ** 2\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\" Overrides the () operator and returns a one-hot encoding of the given state.\"\"\"\n",
    "        phi = np.zeros(self.num_features)\n",
    "        index = np.floor((state - self._env_low) / self._env_dx * (self.n_states-1))\n",
    "        phi[int(index[0] * self.n_states + index[1])] = 1\n",
    "        return phi\n",
    "\n",
    "class RBF (BasisFunctions):\n",
    "    \"\"\" Exponential radial basis functions in 2d spaces.\"\"\"\n",
    "    normalize = True      # this flag normalizes the L_1 norm of the output vector to 1\n",
    "    centers = None\n",
    "    sigmas = None\n",
    "\n",
    "    def __init__(self, env, n_bases):\n",
    "        \"\"\" Creates a set of equidistant basis functions with n_bases functions in each input dimension. \"\"\"\n",
    "        BasisFunctions.__init__(self, env)\n",
    "        self.name = \"RBF\"\n",
    "        self.num_features = n_bases ** 2\n",
    "        self._make_centers(n_bases)\n",
    "    \n",
    "    def _make_centers(self, n_bases):\n",
    "        \"\"\" Initializes the centers of the RBF basis functions as an equidistant grid \n",
    "            and width sigma as the distance between the centers to guarantee enough overlap. \"\"\"\n",
    "        # Create centers of the RBF\n",
    "        self.centers = np.zeros((2, n_bases ** 2))\n",
    "        self.centers[0, :] = np.repeat(np.linspace(self._env_low[0], self._env_high[0], num=n_bases), n_bases)\n",
    "        self.centers[1, :] = np.tile(np.linspace(self._env_low[1], self._env_high[1], num=n_bases), n_bases)\n",
    "        # Create widths of the RBF\n",
    "        self.sigmas = np.zeros((2, n_bases ** 2))\n",
    "        self.sigmas[0, :] = np.repeat(np.ones(n_bases) * self._env_dx[0] / (n_bases-1), n_bases)\n",
    "        self.sigmas[1, :] = np.tile(np.ones(n_bases)  * self._env_dx[1] / (n_bases-1), n_bases)\n",
    "\n",
    "    def __call__(self, state):\n",
    "        \"\"\" Overrides the () operator and returns the (normalised) RBF output for the given state.\"\"\"\n",
    "        # Compute the RBFs\n",
    "        phi = np.exp(- np.sum(((self.centers - np.expand_dims(state, axis=1)) / self.sigmas) ** 2, axis=0))\n",
    "        # Optionally normalize the RBF output to sum 1\n",
    "        if self.normalize:\n",
    "            phi /= np.sum(phi, axis=0)\n",
    "        # Return feature(s)\n",
    "        return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b: semi-gradients \n",
    "Derive the semi-gradient of the quadratic Bellman-error of Q-learning at time $t$ (i.e. after observing s_t, a_t, r_t and s_{t+1}) for a linear Q-function. Write down both the loss function and the semi-gradient as a LaTeX formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%latex\n",
    "The Q-value function is given by $Q(s,a) = \\vec w^{a} \\!\\!\\cdot \\vec \\phi(s)$.\n",
    "\\begin{eqnarray*} \n",
    "    \\mathcal{L}_t &=& \\ldots\n",
    "\\\\[2mm]\n",
    "    \\frac{\\partial \\mathcal{L}_t}{\\partial \\vec w^{a}} &=& \\ldots\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2c: linear Q-functions \n",
    "Implement the corresponding semi-gradient QLearner in the below skeleton. Make sure that, similarly to task 1a, the end of the episode (done=True) is handled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQLearner (QLearner):\n",
    "    learn_rate = 0.1\n",
    "    basis = None\n",
    "    # ...\n",
    "\n",
    "    def __init__(self, env, basis: BasisFunctions):\n",
    "        self.name = \"Linear (%u %s)\" % (basis.num_features, basis.name)\n",
    "        self.basis = basis\n",
    "        # ...\n",
    "\n",
    "    def q_values(self, state):\n",
    "        \"\"\" Returns the estimated Q-values (as a np.ndarray) of the given state. \"\"\"\n",
    "        pass # ...\n",
    "\n",
    "    def sample(self, state):\n",
    "        \"\"\" Returns an action the agent has chosen at the given state. \"\"\"\n",
    "        return 0 # ...\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\" The agent 'learns' from the given transition. \"\"\"\n",
    "        pass # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2d: approximation with one-hot bases \n",
    "Test your above implementation with one-hot basis functions (with 40 bases in each state-dimension). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, LinearQLearner(env, OneHot(env, 40)))\n",
    "plot_all_results(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent on one-hot vectors are equivalent to tabular Q-learning. Do your results mirror the performance of the TabularQLearner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e: approximation with RBF bases\n",
    "Now test your LinearQLearner with the given RBF bases (15 centers for each state dimension). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, LinearQLearner(env, RBF(env, 16)))\n",
    "plot_all_results(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function approximation allows the agent to learn much faster, but the agent's performance can become extremly unstable in later episodes. This is somewhat surprising, as the lecture has established that semi-gradient TD(0) learning is supposed to converge in the linear case. Discuss with your neighbors why this is not the case here. If you want, change the number of episodes in run_experiment() to a value at which your algorithm is still stable and compare the resulting values/policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 3: Stabilising Approximate Q-learning\n",
    "Q-learning with semi-gradient descent is often unstable. In this exercise you will implement techniques that have emerged to stabilise approximate RL, namely Target Networks and Experience Replay Buffers.\n",
    "\n",
    "## Task 3a: target networks\n",
    "In this exercise you will improve the stability of gradient descend by using target networks. Note that for our purposes, the term \"target network\" refers to another weight vector. Extend the LinearQLearner with a target network, which is periodically updated with the current weights every 100 gradient steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetQLearner (LinearQLearner):\n",
    "    # ...\n",
    "    \n",
    "    def __init__(self, env, basis):\n",
    "        LinearQLearner.__init__(self, env, basis)\n",
    "        self.name = \"Target (%u %s)\" % (basis.num_features, basis.name)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your above TargetQLearner with the same 16x16 RBF bases as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, TargetQLearner(env, RBF(env, 16)))\n",
    "plot_all_results(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target networks can stabilise an agent's performance quite a bit, but do not have to. Do you observe more stable learning? Is the behaviour still unstable at the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3b: double Q-learning\n",
    "Extend the class TargetQLearner to implement double Q-learning (akin to Double DQN from the lecture). Double Q-learning determines the best action for the next state s_{t+1} using the current network, but evaluates that action on the target network, just as in TargetQLearner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearner (TargetQLearner):\n",
    "    # ...\n",
    "    \n",
    "    def __init__(self, env, basis):\n",
    "        TargetQLearner.__init__(self, env, basis)\n",
    "        self.name = \"Double (%u %s)\" % (basis.num_features, basis.name)\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test your above DoubleQLearner with the same RBF bases as above. Double Q-learning should further stabilise the agent's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(env, DoubleQLearner(env, RBF(env, 16)))\n",
    "plot_all_results(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have a stable solution by now, increase the number of basis fuctions for each state dimension.\n",
    "How does the policy look like in comparison to the tabular case in task 1c? Can you formulate an optimal policy for mountain-car based on your observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 4: Deep Q-Networks\n",
    "In this exercise we will implement Q-learning with a neural network. Read the DQN paper by Mnih et al. https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf for help with the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4a\n",
    "Implement a deep Q net that holds 10,000 transitions and uses a mini-batch of 32 samples. Make sure that the mini-batch always includes the newest transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code describes a simple replay buffer which is used to store (state, action, reward, next_state, done) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        for i in range(capacity):\n",
    "                self.buffer.append(None)\n",
    "        self.capacity = capacity\n",
    "        self.insert_index = 0\n",
    "        self.num_exp = 0\n",
    "\n",
    "    def push(self, sarst):\n",
    "        self.buffer[self.insert_index] = sarst\n",
    "        self.insert_index = (self.insert_index + 1)%self.capacity\n",
    "        if self.num_exp < self.capacity:\n",
    "            self.num_exp += 1\n",
    "\n",
    "    def sample_buffer(self):\n",
    "        index = random.randint(0, self.num_exp-1)\n",
    "        return self.buffer[index]\n",
    "\n",
    "    def sample_batch(self, size):\n",
    "        batch = []\n",
    "        for i in range(size):\n",
    "            batch.append(self.sample_buffer())\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly the code-let below describes simple a 3 layer neural network which outputs an action value for all the valid actions in mountaincar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value_Net(torch.nn.Module):\n",
    "    hidden = 20\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Value_Net, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2, self.hidden)\n",
    "        self.fc2 = torch.nn.Linear(self.hidden, self.hidden)\n",
    "        self.fc3 = torch.nn.Linear(self.hidden, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the class DQLearner to implement a DQN that solves the mountain car environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLearner(QLearner):\n",
    "    buffer_size = 10000\n",
    "    batch_size = 32\n",
    "    learning_rate = 1e-3\n",
    "    qnet = None\n",
    "    buffer = None\n",
    "    optimizer = None\n",
    "    criterion = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.name = \"DQN\"\n",
    "        self.qnet = Value_Net()\n",
    "        self.buffer = Replay_buffer(self.buffer_size)\n",
    "        self.optimizer = torch.optim.RMSprop(self.qnet.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def q_values(self, state):\n",
    "        return self.qnet(torch.Tensor(state))\n",
    "\n",
    "    def sample(self, state):\n",
    "        return np.argmax(self.q_values(state).detach().numpy())\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        obs = {'old': state, 'action': action, 'reward': reward, 'new': next_state, 'done': done}\n",
    "        self.buffer.push(obs)\n",
    "        if self.buffer.num_exp < self.batch_size:\n",
    "            return\n",
    "        else:\n",
    "            # Sample a batch of transitions (which includes the currently observed)\n",
    "            batch = self.buffer.sample_batch(self.batch_size - 1)\n",
    "            batch.append(obs)\n",
    "            # ...\n",
    "            # Compute Q leanring loss for the sampled batch\n",
    "            # Reset the gradients to zero\n",
    "            # Backpropagate loss gradient through the network\n",
    "            # Take a parameter update step\n",
    "            return\n",
    "    \n",
    "    def set_epsilon(self, iter):\n",
    "        \"\"\" Exponentially decays the eploration parameter epsilon. \"\"\"\n",
    "        self.epsilon = nate**(-iter/250.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_experiment(env, DQLearner())\n",
    "plot_all_results(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4b: DQN variations\n",
    "There are several things that can be tried out with the DQN. Figure out if using different learning rates, losses and network architecture (number of layers and connectivity) changes the network performance. You can even change the replay buffer implementation to define some heuristic to draw samples from. Explore if these modifications help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 5: Improve Q-learning\n",
    "If you have finished the above tasks and still have some time left, play aound with your solutions to improve performance. In particular, test the following changes for any of the above classes that inherit from LinearQLearner:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5a: number of basis function\n",
    "Plot the performance of the LinearQLearner (and it's descendants) against the number of used RBF basis functions in [10x10, ..., 20x20]. Does an increasing number of bases increase or decrease stability? Note that you need to write your own plotting script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5b: learning rate\n",
    "Change the implementation of LinearQLearner (and it's descendants) to decrease the learning rate slowly. How does this change affect learning rate and stability? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5c: exploration\n",
    "The experiment defined in task 1b uses epsilon-greedy exploration with a constant epsilon=0.1. Override the set_epsilon method of (a subclass of) LinearQLearner to implement a decay shedule, which decreases epsilon linearly from 1 to 0.01 in {10, 50, 100, 500} epsiodes. Does this improve the performance or not? What happens with an exponential decay schedule as in DQLearner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5e: scientific evaluation\n",
    "The experiment defined in task 1b fixes the random seed to ensure reproducability of results. However, each algorithm can behave very different depending on the seed. Run one (or all) above experiment(s) with 10 random seeds (i.e. comment out the lines that set the seed to 0) and plot the mean and staqndard deviation of the received rewards. Note that you may want to write your own plotting script, in which the shaded area refers to another type of standard deviation (between seeds, not over time) than in the given script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
